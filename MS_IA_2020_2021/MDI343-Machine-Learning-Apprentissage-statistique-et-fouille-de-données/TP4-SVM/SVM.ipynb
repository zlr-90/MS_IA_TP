{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pshSaZesUmmE"
   },
   "source": [
    "# **Support Vector Machines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNcS9eL7Umh6"
   },
   "source": [
    "\n",
    "\n",
    "Il s'agit de techniques d'apprentissage supervisé pour résoudre des problèmes de classification et de régression (données possiblement non séparable linéairement).\n",
    "SVM peut être vu comme généralisation des classifieurs linéaires (séparer l'espace par un hyperplan affine).\n",
    "\n",
    "**Pros:**\n",
    "*   robuste à grande dimension\n",
    "*   garanties théoriques\n",
    "*   bons résultats en pratique\n",
    "*   faible nombre d'hyperparamètres\n",
    "\n",
    "**Objectif:** construire fonction de décision $\\hat{f}: \\mathcal{X} \\longrightarrow \\{-1, 1\\}$ (classification binaire) pour prédire étiquette pour point inconnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20gV0TDtEhFU"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Rappels:**\n",
    "\n",
    "*   espace d'Hilbert: $(\\mathcal{H}, <\\cdot,\\cdot>_{\\mathcal{H}})$ complet pour la norme induite (complet: toute suite de Cauchy converge dans $\\mathcal{H}$)\n",
    "*   kernel: fonction symétrique, semi définie positive\n",
    "\n",
    "\n",
    "*   Primal:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "& \\min\\limits_w f(w) = \\min\\limits_w L(w, \\mu, \\nu)\\\\\n",
    "& s.t.: w \\geq a\\\\\n",
    "& \\quad \\quad \\sum w = b\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "Lagrangien: $$L(w, \\mu, \\nu) = f(w) - \\mu(w-a) + \\nu(\\sum w - b)$$\n",
    "Dual:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "& \\max\\limits_{\\mu, \\nu} L(w, \\mu, \\nu)\\\\\n",
    "& s.t.: \\mu \\geq 0\\\\\n",
    "& \\quad \\quad \\nu \\geq 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DdmirWcUmcI"
   },
   "source": [
    "Les SVM (ou séparateurs à vastes marges) reposent sur deux idées clés:\n",
    "\n",
    "**1.   fonction noyau:**\n",
    "\n",
    "*   motivation: données non linéairement séparables $\\Longrightarrow$ on considère le problème dans un espace de dimension supérieure, où il existe une séparation linéaire\n",
    "*   appliquer aux vecteurs d'entrée $x$ une transformation non linéaire $\\Phi$: $\\mathcal{X}$ devient $(\\mathcal{H}, <\\cdot,\\cdot>_{\\mathcal{H}})$ espace d'Hilbert en plus grande dimension (espace de redescription)\n",
    "*   kernel trick: $K(x, x') = <\\Phi(x), \\Phi(x')>$\n",
    "*   pros: pas besoin de connaître explicitement $\\Phi$, moins coûteux (transforme produit scalaire en grande dimension (coûteux) en simple évaluation en certains points)\n",
    "\n",
    "\n",
    "> **$\\Longrightarrow$** hyperplan: $h(x) = <w, \\Phi(x)> + w_0 \\quad$ et frontière: $\\{x, <w, \\Phi(x)> + w_0 = 0\\}$\n",
    "\n",
    "> **$\\Longrightarrow$** classifieur: $\\hat{f}_{w, w_0}(x) = sign(h(x))$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**2.   marge maximale:**\n",
    "\n",
    "*   motivation: choix hyperplan séparateur parmi plusieurs avec la même performance $\\Longrightarrow$ regarder performance en généralisation des hyperplans\n",
    "*   marge est distance entre frontière de séparation et échantillons les plus proches (appelés vecteurs supports)\n",
    "*   unique hyperplan optimal: hyperplan qui maximise marge entre hyperplan séparateur et échantillons (voir Théorème de Vapnik Chervonenkis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0nQUr9xUmXh"
   },
   "source": [
    "**Problème Primal:**\n",
    "\n",
    "\n",
    "*   distance échantillon $x_i$ à hyperplan est: $\\frac{y_i(<w, \\Phi(x_i)> + w_0)}{||w||}$\n",
    "*   condition de séparabilité: $y_i h(x_i) = y_i(<w, \\Phi(x_i)> + w_0) \\geq 0$\n",
    "\n",
    "$\\Longrightarrow$ hyperplan séparateur de marge maximale:\n",
    "\\begin{equation}\n",
    "\\arg\\max\\limits_w \\{\\frac{1}{||w||} \\min\\limits_i \\{y_i(<w, \\Phi(x_i)> + w_0)\\}\\}\n",
    "\\end{equation}\n",
    "\n",
    "*   hyperplan canonique: normaliser poids (pour faciliter optimisation): $y_i(<w, \\Phi(x_i)> + w_0) \\geq 1$\n",
    "\n",
    "$\\Longrightarrow$ hyperplan séparateur de marge maximale:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "& \\min\\limits_w \\{\\frac{1}{2} ||w||^2\\}\\\\\n",
    "& s.t.: y_i(<w, \\Phi(x_i)> + w_0) \\geq 1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Marge Souple:**\n",
    "\n",
    "\n",
    "*   il est possible de ne pas trouver de séparateur linéaire dans l'espace de redescription\n",
    "*   ajout de variables de ressort $\\xi_i$ pour relacher les contraintes et d'un paramètre $C$ pour contrôler le compromis entre le nombre d'erreurs et la largeur de la marge (en pénalisant les variables de ressort trop élevées)\n",
    "*   choix du paramètre $C$: par utilisateur, recherche exhaustive, validation croisée\n",
    "\n",
    "$\\Longrightarrow$ hyperplan séparateur de marge maximale:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "& \\min\\limits_w \\{\\frac{1}{2} ||w||^2 + C \\sum\\limits_{i=1}^n \\xi_i\\}\\\\\n",
    "& s.t.: \\xi_i \\geq 0\\\\\n",
    "& \\quad \\quad y_i(<w, \\Phi(x_i)> + w_0) \\geq 1 - \\xi_i\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Problème Dual:**\n",
    "\n",
    "\n",
    "*   Lagrangien: $$L(w, w_0, \\alpha) = \\frac{1}{2} ||w||^2 + C \\sum\\limits_{i=1}^n \\xi_i - \\sum\\limits_{i=1}^n \\alpha_i (y_i(<w, \\Phi(x_i)> + w_0) - 1 + \\xi_i)$$\n",
    "*   conditions de Kuhn Tucker: $\\delta L = 0 \\Longleftrightarrow \\sum\\limits_{i=1}^n \\alpha_i y_i \\Phi(x_i) = w^*$ and $\\sum\\limits_{i=1}^n \\alpha_i y_i = 0$\n",
    "\n",
    "$\\Longrightarrow$ formulation duale:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "& \\max\\limits_{\\alpha} \\{L(w^*, w_0, \\alpha) = \\sum\\limits_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum\\limits_{i=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\\}\\\\\n",
    "& s.t.: 0 \\leq \\alpha_i \\leq C\\\\\n",
    "& \\quad \\quad \\sum\\limits_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0SCa6cyUmSF"
   },
   "source": [
    "---\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "solution primal: $w^* = \\sum\\limits_{i=1}^n \\alpha_i y_i \\Phi(x_i)$\n",
    "\n",
    "solution duale: $\\alpha^*$\n",
    "\n",
    "$\\Longrightarrow$ hyperplan solution: $$h(x) = <w^*, \\Phi(x)> + w_0 = \\sum\\limits_{i=1}^n \\alpha_i^* y_i \\Phi(x_i)^T \\Phi(x) + w_0 = \\sum\\limits_{i=1}^n \\alpha_i^* y_i K(x_i, x) + w_0$$\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "d_FkhtBg7pdI",
    "HZwPhzfF77tC",
    "5VNdGNUn8F1f",
    "s9JOUQDf8R1m"
   ],
   "name": "Python_Classes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
