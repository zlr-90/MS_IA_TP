{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "#               Impoort part\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "# Displaying labeled data\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'axes.facecolor': 'white',\n",
       " 'axes.edgecolor': '.15',\n",
       " 'axes.grid': False,\n",
       " 'axes.axisbelow': True,\n",
       " 'axes.labelcolor': '.15',\n",
       " 'figure.facecolor': 'white',\n",
       " 'grid.color': '.8',\n",
       " 'grid.linestyle': '-',\n",
       " 'text.color': '.15',\n",
       " 'xtick.color': '.15',\n",
       " 'ytick.color': '.15',\n",
       " 'xtick.direction': 'out',\n",
       " 'ytick.direction': 'out',\n",
       " 'lines.solid_capstyle': 'round',\n",
       " 'patch.edgecolor': 'w',\n",
       " 'image.cmap': 'rocket',\n",
       " 'font.family': ['sans-serif'],\n",
       " 'font.sans-serif': ['Arial',\n",
       "  'DejaVu Sans',\n",
       "  'Liberation Sans',\n",
       "  'Bitstream Vera Sans',\n",
       "  'sans-serif'],\n",
       " 'patch.force_edgecolor': True,\n",
       " 'xtick.bottom': False,\n",
       " 'xtick.top': False,\n",
       " 'ytick.left': False,\n",
       " 'ytick.right': False,\n",
       " 'axes.spines.left': True,\n",
       " 'axes.spines.bottom': True,\n",
       " 'axes.spines.right': True,\n",
       " 'axes.spines.top': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symlist = ['o', 'p', '*', 's', '+', 'x', 'D', 'v', '-', '^']\n",
    "\n",
    "rc('font', **{'family': 'sans-serif', 'sans-serif': ['Computer Modern Roman']})\n",
    "params = {'axes.labelsize': 12,\n",
    "          'font.size': 16,\n",
    "          'legend.fontsize': 16,\n",
    "          'text.usetex': False,\n",
    "          'figure.figsize': (8, 6)}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style(\"white\")\n",
    "sns.axes_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "#    Data Generation    (you can skip the understanding)\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rand_gauss(n=100, mu=[1, 1], sigmas=[0.1, 0.1]):\n",
    "    \"\"\"Sample  points from a Gaussian variable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : number of samples\n",
    "\n",
    "    mu : centered\n",
    "\n",
    "    sigma : standard deviation\n",
    "    \"\"\"\n",
    "    d = len(mu)\n",
    "    res = np.random.randn(n, d)\n",
    "    return np.array(mu + res * sigmas)\n",
    "\n",
    "\n",
    "def rand_bi_gauss(n1=100, n2=100, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.1, 0.1],\n",
    "                  sigmas2=[0.1, 0.1]):\n",
    "    \"\"\"Sample points from two Gaussian distributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : number of sample from first distribution\n",
    "\n",
    "    n2 : number of sample from second distribution\n",
    "\n",
    "    mu1 : center for first distribution\n",
    "\n",
    "    mu2 : center for second distribution\n",
    "\n",
    "    sigma1: std deviation for first distribution\n",
    "\n",
    "    sigma2: std deviation for second distribution\n",
    "    \"\"\"\n",
    "    ex1 = rand_gauss(n1, mu1, sigmas1)\n",
    "    ex2 = rand_gauss(n2, mu2, sigmas2)\n",
    "    y = np.hstack([np.ones(n1), -1 * np.ones(n2)])\n",
    "    X = np.vstack([ex1, ex2])\n",
    "    ind = np.random.permutation(n1 + n2)\n",
    "    return X[ind, :], y[ind]\n",
    "\n",
    "\n",
    "def rand_clown(n1=100, n2=100, sigma1=1, sigma2=2):\n",
    "    \"\"\"Create samples and labels form a **clown** dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : number of sample from first blob\n",
    "\n",
    "    n2 : number of sample from second blob\n",
    "\n",
    "    sigma1 :  noise std deviation for the first blob\n",
    "\n",
    "    sigma2 :  noise std deviation for the second blob\n",
    "    \"\"\"\n",
    "    x0 = np.random.randn(n1, 1)\n",
    "    x1 = x0 * x0 + sigma1 * np.random.randn(n1, 1)\n",
    "    x2 = np.hstack([sigma2 * np.random.randn(n2, 1),\n",
    "                    sigma2 * np.random.randn(n2, 1) + 2.])\n",
    "    X = np.vstack([np.hstack([x0, x1]), x2])\n",
    "    y = np.hstack([np.ones(n1), -1 * np.ones(n2)])\n",
    "    ind = np.random.permutation(n1 + n2)\n",
    "    return X[ind, :], y[ind]\n",
    "\n",
    "\n",
    "def rand_checkers(n1=100, n2=100, sigma=0.1):\n",
    "    \"\"\"Create samples and labels from a noisy checker.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : number of samples for the first class\n",
    "\n",
    "    n2 : number of samples for the second class\n",
    "    \"\"\"\n",
    "    nbp = int(np.floor(n1 / 8))\n",
    "    nbn = int(np.floor(n2 / 8))\n",
    "    xapp = np.reshape(np.random.rand((nbp + nbn) * 16), [(nbp + nbn) * 8, 2])\n",
    "    yapp = np.ones((nbp + nbn) * 8)\n",
    "    idx = 0\n",
    "    for i in range(-2, 2):\n",
    "        for j in range(-2, 2):\n",
    "            if (((i + j) % 2) == 0):\n",
    "                nb = nbp\n",
    "            else:\n",
    "                nb = nbn\n",
    "                yapp[idx:(idx + nb)] = [-1] * nb\n",
    "\n",
    "            xapp[idx:(idx + nb), 0] = np.random.rand(nb)\n",
    "            xapp[idx:(idx + nb), 0] += i + sigma * np.random.randn(nb)\n",
    "            xapp[idx:(idx + nb), 1] = np.random.rand(nb)\n",
    "            xapp[idx:(idx + nb), 1] += j + sigma * np.random.randn(nb)\n",
    "            idx += nb\n",
    "\n",
    "    ind = np.random.permutation((nbp + nbn) * 8)\n",
    "    res = np.hstack([xapp, yapp[:, np.newaxis]])\n",
    "    return np.array(res[ind, :2]), np.array(res[ind, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "#            Displaying labeled data\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "symlist = ['o', 's', '+', 'x', 'D', '*', 'p', 'v', '-', '^']\n",
    "collist = ['blue', 'red', 'purple', 'orange', 'salmon', 'black', 'grey',\n",
    "           'fuchsia']\n",
    "\n",
    "\n",
    "def plot_2d(X, y, w=None, step=50, alpha_choice=1):\n",
    "    \"\"\"2D dataset data ploting according to labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : data features\n",
    "\n",
    "    y : label vector\n",
    "\n",
    "    w :(optional) the separating hyperplan w\n",
    "\n",
    "    alpha_choice : control alpha display parameter\n",
    "    \"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    labels = np.unique(y)\n",
    "    k = np.unique(y).shape[0]\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", k)\n",
    "    sns.set_palette(color_blind_list)\n",
    "    for i, label in enumerate(y):\n",
    "        label_num = np.where(labels == label)[0][0]\n",
    "        plt.scatter(X[i, 0], X[i, 1],\n",
    "                    c=np.reshape(color_blind_list[label_num], (1, -1)),\n",
    "                    s=80, marker=symlist[label_num])\n",
    "    plt.xlim([min_tot0 - delta0 / 10., max_tot0 + delta0 / 10.])\n",
    "    plt.ylim([min_tot1 - delta1 / 10., max_tot1 + delta1 / 10.])\n",
    "    if w is not None:\n",
    "        plt.plot([min_tot0, max_tot0],\n",
    "                 [min_tot0 * -w[1] / w[2] - w[0] / w[2],\n",
    "                  max_tot0 * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"k\", alpha=alpha_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "#               Loss functions and their gradient\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    \"\"\"Prediction from a normal vector.\"\"\"\n",
    "    return np.dot(x, w[1:]) + w[0]\n",
    "\n",
    "\n",
    "def predict_class(x, w):\n",
    "    \"\"\"Predict a class from at point x thanks to a normal vector.\"\"\"\n",
    "    return np.sign(predict(x, w))\n",
    "\n",
    "\n",
    "def zero_one_loss(x, y, w):\n",
    "    \"\"\"0-1 loss function.\"\"\"\n",
    "    return abs(y - np.sign(predict(x, w))) / 2.\n",
    "\n",
    "\n",
    "def hinge_loss(x, y, w):\n",
    "    \"\"\"Hinge loss function.\"\"\"\n",
    "    return np.maximum(0., 1. - y * predict(x, w))\n",
    "\n",
    "\n",
    "def mse_loss(x, y, w):\n",
    "    \"\"\"Mean square error loss.\"\"\"\n",
    "    return (y - predict(x, w)) ** 2\n",
    "\n",
    "\n",
    "def norm2(x, y, w):\n",
    "    \"\"\"Squared norm of a vector.\"\"\"\n",
    "    return np.dot(w, w)\n",
    "\n",
    "\n",
    "def gr_hinge_loss(x, y, w):\n",
    "    \"\"\"Sub-gradient of the loss function hingeloss.\"\"\"\n",
    "    return np.dot(-y * (hinge_loss(x, y, w) > 0.),\n",
    "                  np.hstack((np.ones((x.shape[0], 1)), x)))\n",
    "\n",
    "\n",
    "def gr_mse_loss(x, y, w):\n",
    "    \"\"\"Gradient of the least squares lost function.\"\"\"\n",
    "    return -2. * np.dot(y - predict(x, w),\n",
    "                        np.hstack((np.ones((x.shape[0], 1)), x)))\n",
    "\n",
    "def gr_norm2(x, y, w):\n",
    "    \"\"\"Gradient of the squared norm.\"\"\"\n",
    "    return 2. * w\n",
    "\n",
    "\n",
    "def pen_loss_aux(x, y, w, l):\n",
    "    \"\"\"Loss function penalized by hinge loss.\"\"\"\n",
    "    return hinge_loss(x, y, w) + l * norm2(x, y, w)\n",
    "\n",
    "\n",
    "def gr_pen_loss_aux(x, y, w, l):\n",
    "    \"\"\"Gradient of hinge loss penalized loss function.\"\"\"\n",
    "    return gr_hinge_loss(x, y, w) + l * gr_norm2(x, y, w, )\n",
    "\n",
    "\n",
    "def pen_loss(l):\n",
    "    \"\"\"Penalized loss function.\"\"\"\n",
    "    return lambda x, y, w: pen_loss_aux(x, y, w, l)\n",
    "\n",
    "\n",
    "def gr_pen_loss(l):\n",
    "    \"\"\"Gradient penalized loss function.\"\"\"\n",
    "    return lambda x, y, w: gr_pen_loss_aux(x, y, w, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "#            Displaying tools for the Frontiere\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frontiere(f, X, step=50, cmap_choice=cm.coolwarm):\n",
    "    \"\"\"Frontiere plotting for a decision function f.\"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot0, max_tot0, delta0 / step),\n",
    "                         np.arange(min_tot1, max_tot1, delta1 / step))\n",
    "    z = np.array([f(vec) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.imshow(z, origin='lower', interpolation=\"nearest\", cmap=cmap_choice,\n",
    "               extent=[min_tot0, max_tot0, min_tot1, max_tot1])\n",
    "    plt.colorbar()\n",
    "\n",
    "def frontiere_new(clf, X, y, w=None, step=50, alpha_choice=1, colorbar=True,\n",
    "                  samples=True, n_labels=3, n_neighbors=3):\n",
    "    \"\"\"Trace la frontiere pour la fonction de decision de clf.\"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot0, max_tot0, delta0 / step),\n",
    "                         np.arange(min_tot1, max_tot1, delta1 / step))\n",
    "    XX = np.c_[xx.ravel(), yy.ravel()]\n",
    "    print(XX.shape)\n",
    "    z = clf.predict(XX)\n",
    "    z = z.reshape(xx.shape)\n",
    "    labels = np.unique(z)\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", labels.shape[0])\n",
    "    my_cmap = ListedColormap(color_blind_list)\n",
    "    plt.imshow(z, origin='lower', interpolation=\"mitchell\", alpha=0.80,\n",
    "               cmap=my_cmap, extent=[min_tot0, max_tot0, min_tot1, max_tot1])\n",
    "    if colorbar is True:\n",
    "        ax = plt.gca()\n",
    "        cbar = plt.colorbar(ticks=labels)\n",
    "        cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "    # color_blind_list = sns.color_palette(\"colorblind\", labels.shape[0])\n",
    "    # sns.set_palette(color_blind_list)\n",
    "    ax = plt.gca()\n",
    "    if samples is True:\n",
    "        for i, label in enumerate(y):\n",
    "            label_num = np.where(labels == label)[0][0]\n",
    "            plt.scatter(X[i, 0], X[i, 1], c=[color_blind_list[label_num]],\n",
    "                        s=80, marker=symlist[label_num])\n",
    "    plt.xlim([min_tot0, max_tot0])\n",
    "    plt.ylim([min_tot1, max_tot1])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    if w is not None:\n",
    "        plt.plot([min_tot0, max_tot0],\n",
    "                 [min_tot0 * -w[1] / w[2] - w[0] / w[2],\n",
    "                  max_tot0 * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"k\", alpha=alpha_choice)\n",
    "    plt.title(\"L=\" + str(n_labels) + \",k=\" +\n",
    "              str(n_neighbors))\n",
    "\n",
    "def frontiere_3d(f, data, step=20):\n",
    "    \"\"\"Plot the 3d frontiere for the decision function f.\"\"\"\n",
    "    ax = plt.gca(projection='3d')\n",
    "    xmin, xmax = data[:, 0].min() - 1., data[:, 0].max() + 1.\n",
    "    ymin, ymax = data[:, 1].min() - 1., data[:, 1].max() + 1.\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, (xmax - xmin) * 1. / step),\n",
    "                         np.arange(ymin, ymax, (ymax - ymin) * 1. / step))\n",
    "    z = np.array([f(vec) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    z = z.reshape(xx.shape)\n",
    "    ax.plot_surface(xx, yy, z, rstride=1, cstride=1,\n",
    "                    linewidth=0., antialiased=False,\n",
    "                    cmap=plt.cm.coolwarm)\n",
    "\n",
    "def plot_cout(X, y, loss_fun, w=None):\n",
    "    \"\"\"Plot the cost function encoded by loss_fun,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : data features\n",
    "    y :  labels\n",
    "    loss_fun : loss function\n",
    "    w : (optionnal) can be used to give a historic path of the weights \"\"\"\n",
    "    def _inter(wn):\n",
    "        ww = np.zeros(3)\n",
    "        ww[1:] = wn\n",
    "        return loss_fun(X, y, ww).mean()\n",
    "    datarange = np.array([[np.min(X[:, 0]), np.min(X[:, 1])],\n",
    "                          [np.max(X[:, 0]), np.max(X[:, 1])]])\n",
    "    frontiere(_inter, np.array(datarange))\n",
    "    if w is not None:\n",
    "        plt.plot(w[:, 1], w[:, 2], 'k')\n",
    "    plt.xlim([np.min(X[:, 0]), np.max(X[:, 0])])\n",
    "    plt.ylim([np.min(X[:, 1]), np.max(X[:, 1])])\n",
    "\n",
    "def plot_cout3d(x, y, loss_fun, w):\n",
    "    \"\"\" trace le cout de la fonction cout loss_fun passee en parametre, en x,y,\n",
    "        en faisant varier les coordonnees du poids w.\n",
    "        W peut etre utilise pour passer un historique de poids\"\"\"\n",
    "    def _inter(wn):\n",
    "        ww = np.zeros(3)\n",
    "        ww[1:] = wn\n",
    "        return loss_fun(x, y, ww).mean()\n",
    "\n",
    "    datarange = np.array([[w[:, 1].min(), w[:, 2].min()],\n",
    "                         [w[:, 1].max(), w[:, 2].max()]])\n",
    "    frontiere_3d(_inter, np.array(datarange))\n",
    "    plt.plot(w[:, 1], w[:, 2], np.array([_inter(w[i, 1:]) for i in\n",
    "             range(w.shape[0])]), 'k-', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "#                Algorithms and functions\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, eps, niter, w_ini, loss_fun, gr_loss_fun, stochastic=True):\n",
    "    \"\"\" algorithme de descente du gradient:\n",
    "        - x : donnees\n",
    "        - y : label\n",
    "        - eps : facteur multiplicatif de descente\n",
    "        - niter : nombre d'iterations\n",
    "        - w_ini\n",
    "        - loss_fun : fonction de cout\n",
    "        - gr_loss_fun : gradient de la fonction de cout\n",
    "        - stoch : True : gradient stochastique\n",
    "        \"\"\"\n",
    "    w = np.zeros((niter, w_ini.size))\n",
    "    w[0] = w_ini\n",
    "    loss = np.zeros(niter)\n",
    "    loss[0] = loss_fun(x, y, w[0]).mean()\n",
    "    for i in range(1, niter):\n",
    "        if stochastic:  # this is for Stochastic Gradient Descent\n",
    "            idx = [np.random.randint(x.shape[0])]\n",
    "        else:           # this is for pure Gradient Descent\n",
    "            idx = np.arange(x.shape[0])\n",
    "        w[i, :] = w[i - 1, :] - eps * gr_loss_fun(x[idx, :],\n",
    "                                                  y[idx], w[i - 1, :])\n",
    "        loss[i] = loss_fun(x, y, w[i, :]).mean()\n",
    "    return w, loss\n",
    "\n",
    "def plot_gradient(X, y, wh, cost_hist, loss_fun):\n",
    "    \"\"\" display 4 figures on how  (stochastic) gradient descent behaves\n",
    "    wh : solution history\n",
    "    cost_hist : cost history\n",
    "    loss_fun : loss function\n",
    "    \"\"\"\n",
    "    best = np.argmin(cost_hist)\n",
    "    plt.subplot(221)\n",
    "    plt.title('Data and hyperplane estimated')\n",
    "    plot_2d(X, y, wh[best, :])\n",
    "    plt.subplot(222)\n",
    "    plt.title('Projection of level line and algorithm path')\n",
    "    plot_cout(X, y, loss_fun, wh)\n",
    "    plt.subplot(223)\n",
    "    plt.title('Objective function vs iterations')\n",
    "    plt.plot(range(cost_hist.shape[0]), cost_hist)\n",
    "    plt.subplot(224, projection='3d')\n",
    "    plt.title('Level line and algorithm path')\n",
    "    plot_cout3d(X, y, loss_fun, wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################################################################\n",
    "#                Polynomial transformations\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2(x):\n",
    "    \"\"\" creates features for second order interactions \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    nb, d = x.shape\n",
    "    res = x\n",
    "    for i in range(0, d):\n",
    "        for j in range(i, d):\n",
    "            res = np.hstack((res, x[:, i:i + 1] * x[:, j:j + 1]))\n",
    "    return res\n",
    "\n",
    "def poly3(x):\n",
    "    \"\"\" creates features for third order interactions \"\"\"\n",
    "    if x.ndim == 1:\n",
    "            x = x[None, :]\n",
    "    nb, d = x.shape\n",
    "    res = poly2(x)\n",
    "    for i in range(0, d):\n",
    "        for j in range(i, d):\n",
    "            for k in range(j, d):\n",
    "                res = np.hstack(\n",
    "                    (res, x[:, i:i + 1] * x[:, j:j + 1] * x[:, k:k + 1]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
