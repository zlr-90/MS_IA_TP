{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrôle de version\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "# Packages nécessaires\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "# A la première utilisation de nltk, télécharger les données nécessaires\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (<span style=\"color:red\">TODO</span> pour signaler du contenu manquant)\n",
    "\n",
    "##  Word Embeddings : Représentations distribuées via l'hypothèse distributionelle\n",
    "\n",
    "**But**: On va chercher à obtenir des représentations denses (comme vecteurs de nombres réels) de mots (et éventuellement de phrases). Ces représentations ont vocation à être distribuées: ce sont des représentations non-locales. On représente un objet comme une combinaison de *features*, par opposition à l'attribution d'un symbole dédié: voir le travail fondateur d'entre autres, Geoffrey Hinton, sur le sujet: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n",
    "\n",
    "Le terme de représentation *distribuées* est très général, mais correspond à que l'on cherche à obtenir. L'enjeu est donc de pouvoir construire, automatiquement, de telles représentations.\n",
    "\n",
    "**Idée sous-jacente**: Elle est basée sur l'hypothèse distributionelle: les informations contextuelles suffisent à obtenir une représentation viable d'objets linguistiques.\n",
    " - *“For a large class of cases [...] the  meaning  of a word is  its  use in the  language.”* Wittgenstein (Philosophical Investigations, 43 - 1953)\n",
    " - *“You shall know a word by the company it keeps”*, Firth (\"A synopsis of linguistic theory 1930-1955.\" - 1957)\n",
    "\n",
    "Ainsi, on peut caractériser un mot par les mots qui l'accompagnent, via des comptes de co-occurences. Deux mots ayant un sens similaire auront une distribution contextuelle similaire et auront donc plus de chance d'apparaître dans des contextes similaires. Cette hypothèse peut servir de justification à l'application de statistiques à la sémantique (extraction d'information, analyse sémantique). Elle permet aussi une certaine forme de généralisation: on peut supposer que les informations que l'on a à propos d'un mot se généraliseront aux mots à la distribution similaire. \n",
    "\n",
    "**Motivation**: On cherche à obtenir des représentations distribuées pour pouvoir, de manière **efficace**:\n",
    "- Directement réaliser une analyse sémantique de surface.\n",
    "- S'en servir comme source d'informations pour d'autres modèles et applications liées au language, notamment pour l'analyse de sentiments. \n",
    "\n",
    "\n",
    "**Terminologie**: Attention à ne pas confondre l'idée de représentation *distribuée* et *distributionelle*. Le second indique en général (pour les mots) que la représentation a été obtenue strictement à partir de comptes de co-occurences, alors qu'on pourra utiliser des informations supplémentaires (labels de documents, tags de partie du discours, ...) pour construire des représentations distribuées. \n",
    "Les modèles qui permettent de construire ces représentations denses, sous forme de vecteurs, sont souvent appellés *vector spaces models*. On appelle aussi régulièrement ces représentations des *word embeddings*, car les mots sont embarqués (*embedded*) dans un espace vectoriel. En Français, on rencontre souvent le terme *plongements de mots* ou *plongements lexicaux*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenir une représentation: comptes d'occurences et de co-occurences\n",
    "\n",
    "Selon le type de corpus dont on dispose, on pourra obtenir différents types d'informations distributionelles. Si l'on a accès à une collection de documents, on pourra ainsi choisir de compter le nombre d'occurence de chaque mot dans chacun des documents, pour obtenir une matrice $mots \\times documents$: c'est sur ce principe qu'est construit **Tf-Idf** (vu au TP précédent). On va maintenant s'intéresser à un cas plus général: on dispose d'une grande quantité de données sous forme de texte, et on cherche à obtenir des représentations de mots sous forme de vecteurs de taille réduite, sans avoir besoin d'un découpage en documents ou catégories. \n",
    "\n",
    "Supposons qu'on dispose d'un corpus contenant $T$ mots différents. On va construire une matrice $\\mathbf{M}$ de taille $T \\times T$ qui contiendra le nombre de co-occurences entre les mots. Il y aura différents facteurs à considérer lors de la construction de cette matrice: \n",
    "- Comment définir le 'contexte' d'un mot, qui permettra de dire que les termes qu'il contient co-occurent avec ce mot ? On pourra choisir d'utiliser différentes échelles: le document, la phrase, le groupe nominal, ou tout simplement une fenêtre de $k$ mots, selon les informations que l'on cherche à capturer.\n",
    "*Encore une fois, si par exemple notre corpus est divisé en $D$ documents, on pourra même s'intéresser aux liens distributionnels entre mots et documents: chacun de ces $D$ documents agira comme un \"contexte\", et on construit une matrice d'occurences $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$.* \n",
    "- Comment quantifier l'importance des comptes ? Par exemple, on pourra donner un poids décroissant à une co-occurence selon la distance entre les deux mots concernés ($\\frac{1}{d+1}$ pour une séparation par $d$ mots).\n",
    "- Faut-il garder tous les mots qui apparaissent dans le corpus ? En général, non. On verra que pour les grands corpus, le nombre de mots différents $T$ est énorme. Deuxièmement, même si le nombre de mots est raisonnable, on ne possèdera que très peu d'information distributionelle sur les mots les plus rares, et la représentation obtenue sera à priori de mauvaise qualité. Il faudra se poser la question de comment filtrer ces mots, et de comment traiter les mots qu'on choisit de ne pas représenter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procédure\n",
    "\n",
    "Pour construire la matrice, on va dans un premier temps recueillir la liste des mots différents (ou le *vocabulaire* $V$) qui apparaissent dans le corpus sous forme de dictionaire {mots -> index}\n",
    "Puis, pour chaque terme $w$ du corpus,\n",
    "- On récupère l'index $i$ correspondant à l'aide de $V$\n",
    "- Pour chaque terme $w'$ du contexte de $w$, \n",
    "  + On récupère l'index $j$ correspondant à l'aide de $V$\n",
    "  + On incrémente $\\mathbf{M}_{i,j}$ par le poids correspondant, ou par $1$. \n",
    "  \n",
    "La procédure est très proche de celle qu'on a suivi au TP précédent, excepté qu'il faut maintenant compter les mots suivant leur apparition \n",
    "  \n",
    "#### Exemple\n",
    "\n",
    "On considère le corpus suivant: \n",
    "\n",
    "*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n",
    "\n",
    "On choisit de définir le contexte d'un mot comme la phrase à laquelle il appartient, et de ne pas utiliser de poids. \n",
    "On obtient la matrice suivante: \n",
    "\n",
    "|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n",
    "|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n",
    "| I             | 0 |      6 |    6 |   2 |         2 |      2 |   2 |    1 |    1 |\n",
    "| the           | 6 |      2 |    7 |   2 |         2 |      3 |   3 |    1 |    1 |\n",
    "| down          | 6 |      7 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n",
    "| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n",
    "| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n",
    "| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n",
    "| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n",
    "| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n",
    "| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"        \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
    "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_HTML.sub(\" \", text)\n",
    "    \n",
    "    tokens = text.split()        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir un Vocabulaire:\n",
    "\n",
    "Cette fois, on va implémenter séparément une fonction retournant le vocabulaire. Il faudra ici pouvoir contrôler sa taille, que ce soit en indiquant un nombre maximum de mots, ou un nombre minimum d'occurences pour qu'on prenne en compte les mots. On ajoute, à la fin, un mot \"inconnu\" qui remplacera tous les mots qui n'apparaissent pas dans notre vocabulaire 'limité'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(corpus, count_threshold=1, voc_threshold=0):\n",
    "    \"\"\"    \n",
    "    Function using word counts to build a vocabulary - can be improved with a second parameter for \n",
    "    setting a frequency threshold\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
    "        voc_threshold (int): maximum size of the vocabulary \n",
    "    Returns:\n",
    "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
    "                                 values: indexes corresponding to each word sorted by frequency        \n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    for sent in corpus:\n",
    "        for word in clean_and_tokenize(sent):\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 0\n",
    "            word_counts[word] += 1    \n",
    "    filtered_word_counts = {word: count for word, count in word_counts.items() if count >= count_threshold}        \n",
    "    words = sorted(filtered_word_counts.keys(), key=word_counts.get, reverse=True)\n",
    "    if voc_threshold > 0:\n",
    "        words = words[:voc_threshold] + ['UNK']   \n",
    "    vocabulary = {words[i] : i for i in range(len(words))}\n",
    "    return vocabulary, {word: filtered_word_counts.get(word, 0) for word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down': 0, 'the': 1, 'i': 2}\n",
      "{'down': 6, 'the': 6, 'i': 5}\n",
      "{'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8}\n",
      "{'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1}\n"
     ]
    }
   ],
   "source": [
    "# Example for testing:\n",
    "\n",
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "\n",
    "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
    "\n",
    "voc, counts = vocabulary(corpus)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir les co-occurences:\n",
    "\n",
    "La fonction prend en entrée le corpus (une liste de strings, correspondant aux documents, ou phrases) et un vocabulaire, ainsi que la taille de la fenêtre de contexte. On pourra aussi implémenter la solution la plus simple - que le contexte d'un mot soit le reste du document duquel il provient. \n",
    "Enfin, on pourra implémenter la possibilité de faire décroitre linéairement l'importance d'un mot du contexte à mesure qu'on s'éloigne du mot d'origine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurence_matrix(corpus, vocabulary, window=0, distance_weighting=False):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        vocabulary (dictionary): words to use in the matrix\n",
    "        window (int): size of the context window; when 0, the context is the whole sentence\n",
    "        distance_weighting (bool): indicates if we use a weight depending on the distance between words for co-oc counts\n",
    "    Returns:\n",
    "        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n",
    "    \"\"\" \n",
    "    l = len(vocabulary)\n",
    "    M = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        sent = clean_and_tokenize(sent)\n",
    "        # Notez l'utilisation de .get, qui permet de récupérer le mot associé (comme avec []) \n",
    "        # tout en renvoyant vers UNK (dernier token du vocabulaire) si le mot n'est pas dans le vocabulaire.\n",
    "        sent_idx = [vocabulary.get(word, len(vocabulary)-1) for word in sent]\n",
    "        for i, idx in enumerate(sent_idx):\n",
    "            if idx > -1:\n",
    "                if window > 0:\n",
    "                    l_ctx_idx = [sent_idx[j] for j in range(max(0,i-window),i)]                \n",
    "                else:\n",
    "                    l_ctx_idx = sent_idx[:i]                \n",
    "                for j, ctx_idx in enumerate(l_ctx_idx):\n",
    "                    if ctx_idx > -1:\n",
    "                        if distance_weighting:\n",
    "                            weight = 1.0 / (len(l_ctx_idx) - j)\n",
    "                        else:\n",
    "                            weight = 1.0\n",
    "                        M[idx, ctx_idx] += weight * 1.0\n",
    "                        M[ctx_idx, idx] += weight * 1.0\n",
    "    return M  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 7. 6. 3. 3. 2. 2. 1. 1.]\n",
      " [7. 2. 6. 2. 2. 3. 3. 1. 1.]\n",
      " [6. 6. 0. 2. 2. 2. 2. 1. 1.]\n",
      " [3. 2. 2. 0. 1. 1. 0. 0. 0.]\n",
      " [3. 2. 2. 1. 0. 0. 0. 1. 0.]\n",
      " [2. 3. 2. 1. 0. 0. 1. 0. 0.]\n",
      " [2. 3. 2. 0. 0. 1. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(co_occurence_matrix(corpus, voc, 0, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 documents\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "filenames_neg = sorted(glob(os.path.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(os.path.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
    "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "\n",
    "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
    "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 93329\n",
      "Part of the corpus by taking the \"x\" most frequent words:\n",
      "5000 : 0.90\n",
      "10000 : 0.94\n",
      "15000 : 0.96\n",
      "20000 : 0.97\n",
      "25000 : 0.98\n",
      "30000 : 0.98\n",
      "35000 : 0.99\n",
      "40000 : 0.99\n",
      "45000 : 0.99\n",
      "50000 : 0.99\n",
      "55000 : 0.99\n",
      "60000 : 0.99\n",
      "65000 : 1.00\n",
      "70000 : 1.00\n",
      "75000 : 1.00\n",
      "80000 : 1.00\n",
      "85000 : 1.00\n",
      "90000 : 1.00\n"
     ]
    }
   ],
   "source": [
    "corpus = texts\n",
    "\n",
    "vocab, word_counts = vocabulary(corpus)\n",
    "rank_counts = {w:[vocab[w], word_counts[w]] for w in vocab}\n",
    "rank_counts_array = np.array(list(rank_counts.values()))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Word counts versus rank')\n",
    "plt.scatter(rank_counts_array[:,0], rank_counts_array[:,1])\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print('Vocabulary size: %i' % len(vocab))\n",
    "print('Part of the corpus by taking the \"x\" most frequent words:')\n",
    "for i in range(5000, len(vocab), 5000):\n",
    "    print('%i : %.2f' % (i, np.sum(rank_counts_array[:i, 1]) / np.sum(rank_counts_array[:,1]) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat de l'analyse: on peut se contenter d'un vocabulaire de 10000, voire 5000 mots - c'est important, car cela va déterminer la taille des objets que l'on va manipuler. On va maintenant recréer la matrice de co-occurence avec différents paramètres. Cela peut-être long: si cela pose problème, travaillez avec un vocabulaire plus réduit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
