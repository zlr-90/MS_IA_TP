{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fa0Cc5cs1R6x",
    "outputId": "9b13f5d2-ffe9-4ff3-d941-5710bf62ac2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n",
      "channels_last\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        204864    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 1)         1601      \n",
      "=================================================================\n",
      "Total params: 839,937\n",
      "Trainable params: 839,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6273      \n",
      "=================================================================\n",
      "Total params: 212,865\n",
      "Trainable params: 212,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epochs: 22\n",
      "Batch size: 128\n",
      "Batches per epoch: 468\n",
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 389/468 [15:09<03:04,  2.34s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import PIL.Image\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "print(keras.__version__)\n",
    "# K.tensorflow_backend.set_image_dim_ordering('th')\n",
    "print(K.image_data_format())\n",
    "\n",
    "# Deterministic output.\n",
    "# Tired of seeing the same results every time? Remove the line below.\n",
    "np.random.seed(1000)\n",
    "\n",
    "# The results are a little better when the dimensionality of the random vector is only 10.\n",
    "# The dimensionality has been left at 100 for consistency with other GAN implementations.\n",
    "randomDim = 100\n",
    "\n",
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "\n",
    "\n",
    "#Set to False to use DCGAN \n",
    "dense=False\n",
    "\n",
    "# Optimizer\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "if dense:\n",
    "  X_train = X_train.reshape(60000, 784)\n",
    "  generator = Sequential()\n",
    "  generator.add(Dense(256, input_dim=randomDim))\n",
    "  generator.add(LeakyReLU(0.2))\n",
    "  generator.add(Dense(512))\n",
    "  generator.add(LeakyReLU(0.2))\n",
    "  generator.add(Dense(1024))\n",
    "  generator.add(LeakyReLU(0.2))\n",
    "  generator.add(Dense(784, activation='tanh'))\n",
    "\n",
    "  discriminator = Sequential()\n",
    "  discriminator.add(Dense(1024, input_dim=784))\n",
    "  discriminator.add(LeakyReLU(0.2))\n",
    "  discriminator.add(Dropout(0.3))\n",
    "  discriminator.add(Dense(512))\n",
    "  discriminator.add(LeakyReLU(0.2))\n",
    "  discriminator.add(Dropout(0.3))\n",
    "  discriminator.add(Dense(256))\n",
    "  discriminator.add(LeakyReLU(0.2))\n",
    "  discriminator.add(Dropout(0.3))\n",
    "  discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "else:\n",
    "  X_train = X_train.reshape(60000, 28,28,1)\n",
    "  generator = Sequential()\n",
    "  generator.add(Dense(128*7*7, input_dim=randomDim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "  #generator.add(BatchNormalization(momentum=0.9)) \n",
    "  generator.add(LeakyReLU(0.2))\n",
    "  generator.add(Reshape(( 7, 7,128)))\n",
    "  generator.add(UpSampling2D(size=(2, 2)))\n",
    "  generator.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
    "  #generator.add(BatchNormalization(momentum=0.9)) \n",
    "  generator.add(LeakyReLU(0.2))\n",
    "  generator.add(UpSampling2D(size=(2, 2)))\n",
    "  generator.add(Conv2D(1, kernel_size=(5, 5), padding='same', activation='tanh'))\n",
    "  generator.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "  discriminator = Sequential()\n",
    "  discriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(28, 28,1), kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "  discriminator.add(LeakyReLU(0.2))\n",
    "  discriminator.add(Dropout(0.3))\n",
    "  discriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
    "  discriminator.add(LeakyReLU(0.2))\n",
    "  discriminator.add(Dropout(0.3))\n",
    "  discriminator.add(Flatten())\n",
    "  discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "generator.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam,metrics=['accuracy'])\n",
    "\n",
    "discriminator.summary()\n",
    "\n",
    "# We combined the generator and discriminator to create a gan model\n",
    "discriminator.trainable = False\n",
    "ganInput = Input(shape=(randomDim,))\n",
    "x = generator(ganInput)\n",
    "ganOutput = discriminator(x)\n",
    "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam,metrics=['accuracy'])\n",
    "\n",
    "dLosses = []\n",
    "gLosses = []\n",
    "\n",
    "# Plot the loss from each batch\n",
    "def plotLoss(epoch):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(dLosses, label='Discriminitive loss')\n",
    "    plt.plot(gLosses, label='Generative loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('gan_loss_epoch_%d.png' % epoch)\n",
    "\n",
    "# Create a wall of generated MNIST images\n",
    "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
    "    generatedImages = generator.predict(noise)\n",
    "    generatedImages = generatedImages.reshape(examples, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generatedImages.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)\n",
    "    IPython.display.display(IPython.display.Image(data=('gan_generated_image_epoch_%d.png' % epoch)))\n",
    "# Save the generator and discriminator networks (and weights) for later use\n",
    "\n",
    "\n",
    "def saveModels(epoch):\n",
    "    generator.save('gan_generator_epoch_%d.h5' % epoch)\n",
    "    discriminator.save('gan_discriminator_epoch_%d.h5' % epoch)\n",
    "\n",
    "epochs=22\n",
    "batchSize=128\n",
    "batchCount = X_train.shape[0] // batchSize\n",
    "print('Epochs:', epochs)\n",
    "print('Batch size:', batchSize)\n",
    "print('Batches per epoch:', batchCount)\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "    for _ in tqdm(range(batchCount)):\n",
    "        ####### Train discriminator #####\n",
    "        \n",
    "\n",
    "        # Get a random set of input noise and images\n",
    "        # Generate fake MNIST images\n",
    "        noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "        \n",
    "        generatedImages = generator.predict(noise)\n",
    "        \n",
    "        #we create a batch composed of real and fake images\n",
    "        imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n",
    "        \n",
    "        X = np.concatenate([imageBatch, generatedImages])\n",
    "\n",
    "        \n",
    "        # We manually create the labels corresponding to X (1 for real, 0 for generated)\n",
    "        yDis = np.zeros(2*batchSize)\n",
    "        yDis[:batchSize] = 1.0\n",
    "\n",
    "        # Train discriminator\n",
    "        discriminator.trainable = True\n",
    "        dloss,acc = discriminator.train_on_batch(X, yDis)\n",
    "        \n",
    "\n",
    "        ####### Train generator #####\n",
    "        # Generate fake MNIST images\n",
    "        noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "        yGen = np.ones(batchSize)\n",
    "        # Train generator\n",
    "\n",
    "        discriminator.trainable = False\n",
    "        gloss,acc = gan.train_on_batch(noise, yGen)\n",
    "        # hist = gan.train_on_batch(noise, yGen)\n",
    "        \n",
    "    # Store loss of most recent batch from this epoch\n",
    "    dLosses.append(dloss)\n",
    "    #gLosses.append(gloss)\n",
    "\n",
    "    if e == 1 or e % 5 == 0:\n",
    "        plotGeneratedImages(e)\n",
    "        saveModels(e)\n",
    "\n",
    "    # Plot losses from every epoch\n",
    "    plotLoss(e)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Correction TP_GAN_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
