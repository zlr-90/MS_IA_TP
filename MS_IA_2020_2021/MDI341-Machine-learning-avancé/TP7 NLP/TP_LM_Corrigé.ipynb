{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A (very small) introduction to pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Tensors are very similar to Numpy arrays, with the added benefit of being usable on GPU. For a short tutorial on various methods to create tensors of particular types, see [this link](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py).\n",
    "The important things to note are that Tensors can be created empty, from lists, and it is very easy to convert a numpy array into a pytorch tensor, and inversely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([140574092781552, 140574091624176, 140574093475312, 140574093475184,\n",
      "        140574017510288])\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.LongTensor(5)\n",
    "b = torch.LongTensor([5])\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2])\n",
    "b = torch.FloatTensor([3])\n",
    "\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interest in us using Pytorch is the ```autograd``` package. ```torch.Tensor```objects have an attribute ```.requires_grad```; if set as True, it starts to track all operations on it. When you finish your computation, can call ```.backward()``` and all the gradients are computed automatically (and stored in the ```.grad``` attribute).\n",
    "\n",
    "One way to easily cut a tensor from the computational once it is not needed anymore is to use ```.detach()```.\n",
    "More info on automatic differentiation in pytorch on [this link](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4873, -0.5195,  0.4941],\n",
      "        [-0.1611,  0.2527,  0.4156]], requires_grad=True)\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([0.3051, 0.2095], requires_grad=True)\n",
      "Initial loss:  1.3340483903884888\n",
      "dL/dw:  tensor([[-0.2969,  0.1286,  1.1893],\n",
      "        [-0.0129, -0.1413,  0.6312]])\n",
      "dL/db:  tensor([0.1414, 0.4640])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "for name, p in linear.named_parameters():\n",
    "    print(name)\n",
    "    print(p)\n",
    "\n",
    "# Build loss function - Mean Square Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('Initial loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after one update:  1.3125115633010864\n"
     ]
    }
   ],
   "source": [
    "# You can perform gradient descent manually, with an in-place update ...\n",
    "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('Loss after one update: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after two updates:  1.2917243242263794\n"
     ]
    }
   ],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of the model.\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# By default, gradients are accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "# is called. Before the backward pass, we need to use the optimizer object to zero all of the\n",
    "# gradients.\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Calling the step function on an Optimizer makes an update to its parameters\n",
    "optimizer.step()\n",
    "\n",
    "# Print out the loss after the second step of gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('Loss after two updates: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a ```Dictionary``` class, that we are going to use to create a vocabulary for our text data. The goal here is to have a convenient tool, with easy access to any information we could need:\n",
    "- A python dictionary ```word2idx``` allowing easy transformation of tokenized text into indexes\n",
    "- A list ```idx2word```, allowing us to find the word corresponding to an index (for interpretation and generation)\n",
    "- A python dictionary ```counter``` used to build the vocabulary, that can provide us with frequency information if needed. \n",
    "- The ```total``` count of words in the dictionary.\n",
    "\n",
    "Important: The data that we are going to use are already pre-processed so we don't need to create special tokens and control the size of the vocabulary ourselves. However, when the text data is raw, methods to preprocess it conveniently should be added here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.counter = {}\n",
    "        self.total = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "            self.counter.setdefault(word, 0)\n",
    "        self.counter[word] += 1\n",
    "        self.total += 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      " \n",
      "\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./wikitext-2/train.txt', 'r', encoding='utf-8') as f:\n",
    "    print(f.readline())\n",
    "    print(f.readline())\n",
    "    print(f.readline())\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\"': 60,\n",
      " '(': 9,\n",
      " ')': 18,\n",
      " ',': 12,\n",
      " '.': 14,\n",
      " '2011': 44,\n",
      " '3': 6,\n",
      " ':': 7,\n",
      " '<unk>': 8,\n",
      " '=': 0,\n",
      " '@-@': 29,\n",
      " 'Battlefield': 17,\n",
      " 'Chronicles': 2,\n",
      " 'Europan': 70,\n",
      " 'Gallia': 67,\n",
      " 'III': 3,\n",
      " 'Imperial': 80,\n",
      " 'January': 43,\n",
      " 'Japan': 24,\n",
      " 'Japanese': 10,\n",
      " 'Media.Vision': 37,\n",
      " 'Nameless': 61,\n",
      " 'PlayStation': 39,\n",
      " 'Portable': 40,\n",
      " 'Raven': 81,\n",
      " 'Released': 41,\n",
      " 'Second': 69,\n",
      " 'Sega': 35,\n",
      " 'Senjō': 4,\n",
      " 'Valkyria': 1,\n",
      " 'War': 71,\n",
      " 'a': 26,\n",
      " 'against': 79,\n",
      " 'and': 36,\n",
      " 'are': 77,\n",
      " 'as': 22,\n",
      " 'black': 75,\n",
      " 'by': 34,\n",
      " 'commonly': 19,\n",
      " 'developed': 33,\n",
      " 'during': 68,\n",
      " 'first': 58,\n",
      " 'follows': 59,\n",
      " 'for': 38,\n",
      " 'fusion': 49,\n",
      " 'game': 32,\n",
      " 'gameplay': 52,\n",
      " 'in': 42,\n",
      " 'is': 25,\n",
      " 'it': 45,\n",
      " 'its': 53,\n",
      " 'lit': 13,\n",
      " 'military': 63,\n",
      " 'nation': 66,\n",
      " 'no': 5,\n",
      " 'of': 15,\n",
      " 'operations': 76,\n",
      " 'outside': 23,\n",
      " 'parallel': 57,\n",
      " 'penal': 62,\n",
      " 'perform': 73,\n",
      " 'pitted': 78,\n",
      " 'playing': 30,\n",
      " 'predecessors': 54,\n",
      " 'real': 50,\n",
      " 'referred': 20,\n",
      " 'role': 28,\n",
      " 'runs': 56,\n",
      " 'same': 48,\n",
      " 'secret': 74,\n",
      " 'series': 47,\n",
      " 'serving': 65,\n",
      " 'story': 55,\n",
      " 'tactical': 27,\n",
      " 'the': 16,\n",
      " 'third': 46,\n",
      " 'time': 51,\n",
      " 'to': 21,\n",
      " 'unit': 64,\n",
      " 'video': 31,\n",
      " 'who': 72,\n",
      " '戦場のヴァルキュリア3': 11}\n",
      "['=',\n",
      " 'Valkyria',\n",
      " 'Chronicles',\n",
      " 'III',\n",
      " 'Senjō',\n",
      " 'no',\n",
      " '3',\n",
      " ':',\n",
      " '<unk>',\n",
      " '(',\n",
      " 'Japanese',\n",
      " '戦場のヴァルキュリア3',\n",
      " ',',\n",
      " 'lit',\n",
      " '.',\n",
      " 'of',\n",
      " 'the',\n",
      " 'Battlefield',\n",
      " ')',\n",
      " 'commonly',\n",
      " 'referred',\n",
      " 'to',\n",
      " 'as',\n",
      " 'outside',\n",
      " 'Japan',\n",
      " 'is',\n",
      " 'a',\n",
      " 'tactical',\n",
      " 'role',\n",
      " '@-@',\n",
      " 'playing',\n",
      " 'video',\n",
      " 'game',\n",
      " 'developed',\n",
      " 'by',\n",
      " 'Sega',\n",
      " 'and',\n",
      " 'Media.Vision',\n",
      " 'for',\n",
      " 'PlayStation',\n",
      " 'Portable',\n",
      " 'Released',\n",
      " 'in',\n",
      " 'January',\n",
      " '2011',\n",
      " 'it',\n",
      " 'third',\n",
      " 'series',\n",
      " 'same',\n",
      " 'fusion',\n",
      " 'real',\n",
      " 'time',\n",
      " 'gameplay',\n",
      " 'its',\n",
      " 'predecessors',\n",
      " 'story',\n",
      " 'runs',\n",
      " 'parallel',\n",
      " 'first',\n",
      " 'follows',\n",
      " '\"',\n",
      " 'Nameless',\n",
      " 'penal',\n",
      " 'military',\n",
      " 'unit',\n",
      " 'serving',\n",
      " 'nation',\n",
      " 'Gallia',\n",
      " 'during',\n",
      " 'Second',\n",
      " 'Europan',\n",
      " 'War',\n",
      " 'who',\n",
      " 'perform',\n",
      " 'secret',\n",
      " 'black',\n",
      " 'operations',\n",
      " 'are',\n",
      " 'pitted',\n",
      " 'against',\n",
      " 'Imperial',\n",
      " 'Raven']\n",
      "{'\"': 4,\n",
      " '(': 1,\n",
      " ')': 1,\n",
      " ',': 6,\n",
      " '.': 4,\n",
      " '2011': 1,\n",
      " '3': 2,\n",
      " ':': 2,\n",
      " '<unk>': 3,\n",
      " '=': 2,\n",
      " '@-@': 2,\n",
      " 'Battlefield': 1,\n",
      " 'Chronicles': 3,\n",
      " 'Europan': 1,\n",
      " 'Gallia': 1,\n",
      " 'III': 2,\n",
      " 'Imperial': 1,\n",
      " 'January': 1,\n",
      " 'Japan': 2,\n",
      " 'Japanese': 1,\n",
      " 'Media.Vision': 1,\n",
      " 'Nameless': 1,\n",
      " 'PlayStation': 1,\n",
      " 'Portable': 1,\n",
      " 'Raven': 1,\n",
      " 'Released': 1,\n",
      " 'Second': 1,\n",
      " 'Sega': 1,\n",
      " 'Senjō': 1,\n",
      " 'Valkyria': 5,\n",
      " 'War': 1,\n",
      " 'a': 2,\n",
      " 'against': 1,\n",
      " 'and': 4,\n",
      " 'are': 1,\n",
      " 'as': 2,\n",
      " 'black': 1,\n",
      " 'by': 1,\n",
      " 'commonly': 1,\n",
      " 'developed': 1,\n",
      " 'during': 1,\n",
      " 'first': 1,\n",
      " 'follows': 1,\n",
      " 'for': 1,\n",
      " 'fusion': 1,\n",
      " 'game': 3,\n",
      " 'gameplay': 1,\n",
      " 'in': 3,\n",
      " 'is': 2,\n",
      " 'it': 1,\n",
      " 'its': 1,\n",
      " 'lit': 1,\n",
      " 'military': 1,\n",
      " 'nation': 1,\n",
      " 'no': 1,\n",
      " 'of': 3,\n",
      " 'operations': 1,\n",
      " 'outside': 1,\n",
      " 'parallel': 1,\n",
      " 'penal': 1,\n",
      " 'perform': 1,\n",
      " 'pitted': 1,\n",
      " 'playing': 1,\n",
      " 'predecessors': 1,\n",
      " 'real': 1,\n",
      " 'referred': 1,\n",
      " 'role': 1,\n",
      " 'runs': 1,\n",
      " 'same': 1,\n",
      " 'secret': 1,\n",
      " 'series': 1,\n",
      " 'serving': 1,\n",
      " 'story': 1,\n",
      " 'tactical': 2,\n",
      " 'the': 11,\n",
      " 'third': 1,\n",
      " 'time': 1,\n",
      " 'to': 2,\n",
      " 'unit': 2,\n",
      " 'video': 1,\n",
      " 'who': 1,\n",
      " '戦場のヴァルキュリア3': 1}\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "# Let's take the four first lines of our training data:\n",
    "corpus = ''\n",
    "with open('./wikitext-2/train.txt', 'r') as f:\n",
    "    for i in range(4):\n",
    "        corpus += f.readline()\n",
    "        \n",
    "# Create an empty Dictionary, separate and add all words. \n",
    "dictio = Dictionary()\n",
    "words = corpus.split()\n",
    "for word in words:\n",
    "    dictio.add_word(word)\n",
    "\n",
    "# Take a look at the objects created:\n",
    "pp.pprint(dictio.word2idx)\n",
    "pp.pprint(dictio.idx2word)\n",
    "pp.pprint(dictio.counter)\n",
    "pp.pprint(dictio.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        # We create an object Dictionary associated to Corpus\n",
    "        self.dictionary = Dictionary()\n",
    "        # We go through all files, adding all words to the dictionary\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "        \n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                tokens += len(words)\n",
    "        \n",
    "        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "data = './wikitext-2/'\n",
    "corpus = Corpus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2551843\n",
      "33278\n",
      "33278\n",
      "torch.Size([2088628])\n",
      "tensor([0, 1, 2, 3, 4, 1, 0])\n",
      "['<eos>', '=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>']\n",
      "torch.Size([217646])\n",
      "tensor([    0,     1, 32966, 32967,     1,     0,     0])\n",
      "['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.dictionary.total)\n",
    "print(len(corpus.dictionary.idx2word))\n",
    "print(len(corpus.dictionary.word2idx))\n",
    "\n",
    "print(corpus.train.shape)\n",
    "print(corpus.train[0:7])\n",
    "print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(7)])\n",
    "\n",
    "print(corpus.valid.shape)\n",
    "print(corpus.valid[0:7])\n",
    "print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have data under a very long list of indexes: the text is as one sequence.\n",
    "# The idea now is to create batches from this. Note that this is absolutely not the best\n",
    "# way to proceed with large quantities of data (where we'll try not to store huge tensors\n",
    "# in memory but read them from file as we go) !\n",
    "# Here, we are looking for simplicity and efficiency with regards to computation time.\n",
    "# That is why we will ignore sentence separations and treat the data as one long stream that\n",
    "# we will cut arbitrarily as we need.\n",
    "# With the alphabet being our data, we currently have the sequence:\n",
    "# [a b c d e f g h i j k l m n o p q r s t u v w x y z]\n",
    "# We want to reorganize it as independant batches that will be processed independantly by the model !\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences:\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘\n",
    "# with the last two elements being lost.\n",
    "# Again, these columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n",
    "\n",
    "def batchify(data, batch_size, cuda = False):\n",
    "    # Cut the elements that are unnecessary\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Reorganize the data\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    # If we can use a GPU, let's tranfer the tensor to it\n",
    "    if cuda:\n",
    "        data = data.cuda()\n",
    "    return data\n",
    "\n",
    "# get_batch subdivides the source data into chunks of the appropriate length.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a sequence length (seq_len) of 3, we'd get the following two variables:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# | b h n t | | c i o u │\n",
    "# └ c i o u ┘ └ d j p v ┘\n",
    "# The first variable contains the letters input to the network, while the second\n",
    "# contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
    "# Note that despite the name of the function, we are cutting the data in the\n",
    "# temporal dimension, since we already divided data into batches in the previous\n",
    "# function. \n",
    "\n",
    "def get_batch(source, i, seq_len, evaluation=False):\n",
    "    # Deal with the possibility that there's not enough data left for a full sequence\n",
    "    seq_len = min(seq_len, len(source) - 1 - i)\n",
    "    # Take the input data\n",
    "    data = source[i:i+seq_len]\n",
    "    # Shift by one for the target data\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20886, 100])\n",
      "torch.Size([54411, 4])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "eval_batch_size = 4\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  1839,   284,   877],\n",
      "        [    1,     9,   613,    17],\n",
      "        [32966,     9,  2123, 11033]])\n",
      "tensor([[    1,     9,   613,    17],\n",
      "        [32966,     9,  2123, 11033],\n",
      "        [32967,     9,    13,  4548]])\n",
      "tensor([[32967,     9,    13,  4548],\n",
      "        [    1,     9,    35,    16],\n",
      "        [    0,     9, 29595,  2582]])\n",
      "tensor([[    1,     9,    35,    16],\n",
      "        [    0,     9, 29595,  2582],\n",
      "        [    0,    10,    13,    37]])\n"
     ]
    }
   ],
   "source": [
    "input_words, target_words = get_batch(val_data, 0, 3)\n",
    "pp.pprint(input_words)\n",
    "pp.pprint(target_words)\n",
    "input_words, target_words = get_batch(val_data, 3, 3)\n",
    "pp.pprint(input_words)\n",
    "pp.pprint(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2373,  0.0900, -0.4898]],\n",
      "\n",
      "        [[ 0.2338,  0.4996, -0.0943]],\n",
      "\n",
      "        [[-0.0330,  0.1991, -0.2932]],\n",
      "\n",
      "        [[ 0.1509,  0.3386, -0.3763]],\n",
      "\n",
      "        [[ 0.2319,  0.0651, -0.2004]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[ 0.2319,  0.0651, -0.2004]]], grad_fn=<StackBackward>),\n",
      " tensor([[[ 0.3897,  0.1889, -0.2667]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# Create a toy example of LSTM: \n",
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# LSTMs expect inputs having 3 dimensions:\n",
    "# - The first dimension is the temporal dimension, along which we (in our case) have the different words\n",
    "# - The second dimension is the batch dimension, along which we stack the independant batches\n",
    "# - The third dimension is the feature dimension, along which are the features of the vector representing the words\n",
    "\n",
    "# In our toy case, we have inputs and outputs containing 3 features (third dimension !)\n",
    "# We created a sequence of 5 different inputs (first dimension !)\n",
    "# We don't use batch (the second dimension will have one lement)\n",
    "\n",
    "# We need an initial hidden state, of the right sizes for dimension 2/3, but with only one temporal element:\n",
    "# Here, it is:\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "# Why do we create a tuple of two tensors ? Because we use LSTMs: remember that they use two sets of weights,\n",
    "# and two hidden states (Hidden state, and Cell state).\n",
    "# If you don't remember, read: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "# If we used a classic RNN, we would simply have:\n",
    "# hidden = torch.randn(1, 1, 3)\n",
    "\n",
    "# The naive way of applying a lstm to inputs is to apply it one step at a time, and loop through the sequence\n",
    "for i in inputs:\n",
    "    # After each step, hidden contains the hidden states (remember, it's a tuple of two states).\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    \n",
    "# Alternatively, we can do the entire sequence all at once.\n",
    "# The first value returned by LSTM is all of the Hidden states throughout the sequence.\n",
    "# The second is just the most recent Hidden state and Cell state (you can compare the values)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence, for each temporal step\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate later, with another sequence\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # Re-initialize\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "pp.pprint(out)\n",
    "pp.pprint(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our own LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are usually implemented as custom nn.Module subclass\n",
    "# We need to redefine the __init__ method, which creates the object\n",
    "# We also need to redefine the forward method, which transform the input into outputs\n",
    "# We can also add any method that we need: here, in order to initiate weights in the model\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Create a dropout object to use on layers for regularization\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # Create an encoder - which is an embedding layer\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        # Create the LSTM layers - find out how to stack them !\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
    "        # (Note that the softmax application function will be applied out of the model)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        \n",
    "        # Initialize non-reccurent weights \n",
    "        self.init_weights()\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialize the encoder and decoder weights with the uniform distribution\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize the hidden state and cell state to zero, with the right sizes\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, batch_size, self.nhid))    \n",
    "\n",
    "    def forward(self, input, hidden, return_h=False):\n",
    "        # Process the input\n",
    "        emb = self.drop(self.encoder(input))   \n",
    "        \n",
    "        # Apply the LSTMs\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        # Decode into scores\n",
    "        output = self.drop(output)      \n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# If you have Cuda installed and a GPU available\n",
    "cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    if not cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably choose cuda = True\")\n",
    "        \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 200\n",
    "layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "vocab_size = len(corpus.dictionary)\n",
    "model = LSTMModel(vocab_size, embedding_size, hidden_size, layers, dropout).to(device)\n",
    "params = list(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 30.0\n",
    "optimizer = 'sgd'\n",
    "wdecay = 1.2e-6\n",
    "# For gradient clipping\n",
    "clip = 0.5\n",
    "\n",
    "if optimizer == 'sgd':\n",
    "    optim = torch.optim.SGD(params, lr=lr, weight_decay=wdecay)\n",
    "if optimizer == 'adam':\n",
    "    optim = torch.optim.Adam(params, lr=lr, weight_decay=wdecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's think about gradient propagation:\n",
    "# We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n",
    "# the next call to LSTM. In this way, we can back-propagate the gradient for as long as we want.\n",
    "# However, this put a huge strain on the memory used by the model, since it implies retaining\n",
    "# a always-growing number of tensors of gradients in the cache.\n",
    "# We decide to not backpropagate through time beyond the current sequence ! \n",
    "# We use a specific function to cut the 'hidden/state cell' states from their previous dependencies\n",
    "# before using them to initialize the next call to the LSTM.\n",
    "# This is done with the .detach() function.\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other global parameters\n",
    "epochs = 10\n",
    "seq_len = 30\n",
    "log_interval = 10\n",
    "save = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
    "            data, targets = get_batch(data_source, i, seq_len)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data) * criterion(output.view(-1, vocab_size), targets.view(-1)).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
    "        data, targets = get_batch(train_data, i, seq_len)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // seq_len, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/  696 batches | lr 30.00 | ms/batch 2507.86 | loss 12.35 | ppl 230338.73\n",
      "| epoch   1 |    20/  696 batches | lr 30.00 | ms/batch 1833.75 | loss  8.38 | ppl  4365.22\n",
      "| epoch   1 |    30/  696 batches | lr 30.00 | ms/batch 1816.29 | loss  7.85 | ppl  2577.99\n",
      "| epoch   1 |    40/  696 batches | lr 30.00 | ms/batch 1844.96 | loss  7.84 | ppl  2534.20\n",
      "| epoch   1 |    50/  696 batches | lr 30.00 | ms/batch 1798.02 | loss  7.67 | ppl  2134.25\n",
      "| epoch   1 |    60/  696 batches | lr 30.00 | ms/batch 1803.66 | loss  7.60 | ppl  1997.20\n",
      "| epoch   1 |    70/  696 batches | lr 30.00 | ms/batch 1800.91 | loss  7.54 | ppl  1876.26\n",
      "| epoch   1 |    80/  696 batches | lr 30.00 | ms/batch 1818.69 | loss  7.46 | ppl  1745.66\n",
      "| epoch   1 |    90/  696 batches | lr 30.00 | ms/batch 1811.97 | loss  7.44 | ppl  1710.45\n",
      "| epoch   1 |   100/  696 batches | lr 30.00 | ms/batch 1782.75 | loss  7.41 | ppl  1655.07\n",
      "| epoch   1 |   110/  696 batches | lr 30.00 | ms/batch 1805.25 | loss  7.36 | ppl  1572.88\n",
      "| epoch   1 |   120/  696 batches | lr 30.00 | ms/batch 1812.01 | loss  7.31 | ppl  1490.24\n",
      "| epoch   1 |   130/  696 batches | lr 30.00 | ms/batch 1828.56 | loss  7.28 | ppl  1445.73\n",
      "| epoch   1 |   140/  696 batches | lr 30.00 | ms/batch 1804.84 | loss  7.19 | ppl  1332.33\n",
      "| epoch   1 |   150/  696 batches | lr 30.00 | ms/batch 1790.64 | loss  7.18 | ppl  1307.88\n",
      "| epoch   1 |   160/  696 batches | lr 30.00 | ms/batch 1861.19 | loss  7.17 | ppl  1302.35\n",
      "| epoch   1 |   170/  696 batches | lr 30.00 | ms/batch 1808.38 | loss  7.12 | ppl  1233.30\n",
      "| epoch   1 |   180/  696 batches | lr 30.00 | ms/batch 1797.86 | loss  6.99 | ppl  1090.65\n",
      "| epoch   1 |   190/  696 batches | lr 30.00 | ms/batch 1830.54 | loss  7.04 | ppl  1139.62\n",
      "| epoch   1 |   200/  696 batches | lr 30.00 | ms/batch 1824.56 | loss  7.02 | ppl  1122.60\n",
      "| epoch   1 |   210/  696 batches | lr 30.00 | ms/batch 1792.83 | loss  6.99 | ppl  1081.20\n",
      "| epoch   1 |   220/  696 batches | lr 30.00 | ms/batch 1827.48 | loss  6.94 | ppl  1033.77\n",
      "| epoch   1 |   230/  696 batches | lr 30.00 | ms/batch 1820.41 | loss  6.95 | ppl  1046.92\n",
      "| epoch   1 |   240/  696 batches | lr 30.00 | ms/batch 1791.10 | loss  6.95 | ppl  1038.48\n",
      "| epoch   1 |   250/  696 batches | lr 30.00 | ms/batch 1797.27 | loss  6.87 | ppl   965.79\n",
      "| epoch   1 |   260/  696 batches | lr 30.00 | ms/batch 1803.04 | loss  6.85 | ppl   940.23\n",
      "| epoch   1 |   270/  696 batches | lr 30.00 | ms/batch 1806.77 | loss  6.95 | ppl  1039.72\n",
      "| epoch   1 |   280/  696 batches | lr 30.00 | ms/batch 1794.37 | loss  6.79 | ppl   891.40\n",
      "| epoch   1 |   290/  696 batches | lr 30.00 | ms/batch 1818.64 | loss  6.81 | ppl   903.06\n",
      "| epoch   1 |   300/  696 batches | lr 30.00 | ms/batch 1782.98 | loss  6.76 | ppl   866.14\n",
      "| epoch   1 |   310/  696 batches | lr 30.00 | ms/batch 1820.18 | loss  6.78 | ppl   876.77\n",
      "| epoch   1 |   320/  696 batches | lr 30.00 | ms/batch 1794.74 | loss  6.79 | ppl   891.95\n",
      "| epoch   1 |   330/  696 batches | lr 30.00 | ms/batch 1794.36 | loss  6.71 | ppl   820.56\n",
      "| epoch   1 |   340/  696 batches | lr 30.00 | ms/batch 1794.35 | loss  6.75 | ppl   854.48\n",
      "| epoch   1 |   350/  696 batches | lr 30.00 | ms/batch 1851.23 | loss  6.70 | ppl   811.76\n",
      "| epoch   1 |   360/  696 batches | lr 30.00 | ms/batch 1819.57 | loss  6.65 | ppl   774.24\n",
      "| epoch   1 |   370/  696 batches | lr 30.00 | ms/batch 1796.88 | loss  6.73 | ppl   834.47\n",
      "| epoch   1 |   380/  696 batches | lr 30.00 | ms/batch 1794.11 | loss  6.66 | ppl   778.07\n",
      "| epoch   1 |   390/  696 batches | lr 30.00 | ms/batch 1798.57 | loss  6.64 | ppl   762.33\n",
      "| epoch   1 |   400/  696 batches | lr 30.00 | ms/batch 1792.94 | loss  6.60 | ppl   737.96\n",
      "| epoch   1 |   410/  696 batches | lr 30.00 | ms/batch 1800.47 | loss  6.64 | ppl   764.55\n",
      "| epoch   1 |   420/  696 batches | lr 30.00 | ms/batch 1818.36 | loss  6.57 | ppl   710.42\n",
      "| epoch   1 |   430/  696 batches | lr 30.00 | ms/batch 1822.59 | loss  6.63 | ppl   754.84\n",
      "| epoch   1 |   440/  696 batches | lr 30.00 | ms/batch 1803.09 | loss  6.52 | ppl   680.01\n",
      "| epoch   1 |   450/  696 batches | lr 30.00 | ms/batch 1804.81 | loss  6.62 | ppl   747.06\n",
      "| epoch   1 |   460/  696 batches | lr 30.00 | ms/batch 1866.31 | loss  6.51 | ppl   674.71\n",
      "| epoch   1 |   470/  696 batches | lr 30.00 | ms/batch 1793.33 | loss  6.55 | ppl   697.62\n",
      "| epoch   1 |   480/  696 batches | lr 30.00 | ms/batch 1777.37 | loss  6.51 | ppl   669.52\n",
      "| epoch   1 |   490/  696 batches | lr 30.00 | ms/batch 1803.84 | loss  6.50 | ppl   663.10\n",
      "| epoch   1 |   500/  696 batches | lr 30.00 | ms/batch 1820.13 | loss  6.45 | ppl   633.23\n",
      "| epoch   1 |   510/  696 batches | lr 30.00 | ms/batch 1791.04 | loss  6.47 | ppl   645.68\n",
      "| epoch   1 |   520/  696 batches | lr 30.00 | ms/batch 1802.44 | loss  6.48 | ppl   652.55\n",
      "| epoch   1 |   530/  696 batches | lr 30.00 | ms/batch 1801.98 | loss  6.42 | ppl   611.02\n",
      "| epoch   1 |   540/  696 batches | lr 30.00 | ms/batch 1816.03 | loss  6.45 | ppl   631.06\n",
      "| epoch   1 |   550/  696 batches | lr 30.00 | ms/batch 1794.37 | loss  6.44 | ppl   624.87\n",
      "| epoch   1 |   560/  696 batches | lr 30.00 | ms/batch 1845.39 | loss  6.37 | ppl   581.81\n",
      "| epoch   1 |   570/  696 batches | lr 30.00 | ms/batch 1805.59 | loss  6.40 | ppl   599.39\n",
      "| epoch   1 |   580/  696 batches | lr 30.00 | ms/batch 1864.44 | loss  6.40 | ppl   599.54\n",
      "| epoch   1 |   590/  696 batches | lr 30.00 | ms/batch 1796.03 | loss  6.43 | ppl   622.73\n",
      "| epoch   1 |   600/  696 batches | lr 30.00 | ms/batch 1793.60 | loss  6.41 | ppl   607.39\n",
      "| epoch   1 |   610/  696 batches | lr 30.00 | ms/batch 1828.01 | loss  6.32 | ppl   557.17\n",
      "| epoch   1 |   620/  696 batches | lr 30.00 | ms/batch 1817.90 | loss  6.40 | ppl   604.12\n",
      "| epoch   1 |   630/  696 batches | lr 30.00 | ms/batch 1782.43 | loss  6.36 | ppl   578.96\n",
      "| epoch   1 |   640/  696 batches | lr 30.00 | ms/batch 1795.84 | loss  6.33 | ppl   563.60\n",
      "| epoch   1 |   650/  696 batches | lr 30.00 | ms/batch 1810.20 | loss  6.34 | ppl   564.87\n",
      "| epoch   1 |   660/  696 batches | lr 30.00 | ms/batch 1801.11 | loss  6.37 | ppl   584.50\n",
      "| epoch   1 |   670/  696 batches | lr 30.00 | ms/batch 1808.16 | loss  6.35 | ppl   572.71\n",
      "| epoch   1 |   680/  696 batches | lr 30.00 | ms/batch 1793.25 | loss  6.34 | ppl   567.88\n",
      "| epoch   1 |   690/  696 batches | lr 30.00 | ms/batch 1828.95 | loss  6.26 | ppl   521.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1325.16s | valid loss  6.13 | valid ppl   457.91\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthieu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTMModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/matthieu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/matthieu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/matthieu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/matthieu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |    10/  696 batches | lr 30.00 | ms/batch 1984.07 | loss  6.88 | ppl   969.45\n",
      "| epoch   2 |    20/  696 batches | lr 30.00 | ms/batch 1801.44 | loss  6.30 | ppl   543.25\n",
      "| epoch   2 |    30/  696 batches | lr 30.00 | ms/batch 1824.87 | loss  6.29 | ppl   540.36\n",
      "| epoch   2 |    40/  696 batches | lr 30.00 | ms/batch 1807.56 | loss  6.32 | ppl   553.02\n",
      "| epoch   2 |    50/  696 batches | lr 30.00 | ms/batch 1796.82 | loss  6.29 | ppl   541.73\n",
      "| epoch   2 |    60/  696 batches | lr 30.00 | ms/batch 1798.36 | loss  6.30 | ppl   544.59\n",
      "| epoch   2 |    70/  696 batches | lr 30.00 | ms/batch 1807.02 | loss  6.26 | ppl   520.75\n",
      "| epoch   2 |    80/  696 batches | lr 30.00 | ms/batch 1800.52 | loss  6.32 | ppl   553.59\n",
      "| epoch   2 |    90/  696 batches | lr 30.00 | ms/batch 1800.40 | loss  6.26 | ppl   523.03\n",
      "| epoch   2 |   100/  696 batches | lr 30.00 | ms/batch 1805.30 | loss  6.29 | ppl   538.37\n",
      "| epoch   2 |   110/  696 batches | lr 30.00 | ms/batch 1813.66 | loss  6.28 | ppl   533.10\n",
      "| epoch   2 |   120/  696 batches | lr 30.00 | ms/batch 1790.29 | loss  6.23 | ppl   509.98\n",
      "| epoch   2 |   130/  696 batches | lr 30.00 | ms/batch 1807.40 | loss  6.19 | ppl   486.49\n",
      "| epoch   2 |   140/  696 batches | lr 30.00 | ms/batch 1796.78 | loss  6.23 | ppl   509.59\n",
      "| epoch   2 |   150/  696 batches | lr 30.00 | ms/batch 1807.11 | loss  6.19 | ppl   486.11\n",
      "| epoch   2 |   160/  696 batches | lr 30.00 | ms/batch 1807.82 | loss  6.14 | ppl   465.23\n",
      "| epoch   2 |   170/  696 batches | lr 30.00 | ms/batch 1790.38 | loss  6.14 | ppl   462.83\n",
      "| epoch   2 |   180/  696 batches | lr 30.00 | ms/batch 1816.07 | loss  6.19 | ppl   486.20\n",
      "| epoch   2 |   190/  696 batches | lr 30.00 | ms/batch 1795.32 | loss  6.21 | ppl   499.21\n",
      "| epoch   2 |   200/  696 batches | lr 30.00 | ms/batch 1791.17 | loss  6.20 | ppl   492.43\n",
      "| epoch   2 |   210/  696 batches | lr 30.00 | ms/batch 1791.60 | loss  6.17 | ppl   475.86\n",
      "| epoch   2 |   220/  696 batches | lr 30.00 | ms/batch 1811.17 | loss  6.14 | ppl   461.80\n",
      "| epoch   2 |   230/  696 batches | lr 30.00 | ms/batch 1806.42 | loss  6.24 | ppl   511.79\n",
      "| epoch   2 |   240/  696 batches | lr 30.00 | ms/batch 1794.82 | loss  6.16 | ppl   473.30\n",
      "| epoch   2 |   250/  696 batches | lr 30.00 | ms/batch 1803.21 | loss  6.15 | ppl   466.99\n",
      "| epoch   2 |   260/  696 batches | lr 30.00 | ms/batch 1813.35 | loss  6.11 | ppl   449.52\n",
      "| epoch   2 |   270/  696 batches | lr 30.00 | ms/batch 1808.97 | loss  6.15 | ppl   469.10\n",
      "| epoch   2 |   280/  696 batches | lr 30.00 | ms/batch 1815.86 | loss  6.10 | ppl   445.74\n",
      "| epoch   2 |   290/  696 batches | lr 30.00 | ms/batch 1803.22 | loss  6.14 | ppl   465.80\n",
      "| epoch   2 |   300/  696 batches | lr 30.00 | ms/batch 1808.33 | loss  6.11 | ppl   451.65\n",
      "| epoch   2 |   310/  696 batches | lr 30.00 | ms/batch 1791.62 | loss  6.08 | ppl   438.99\n",
      "| epoch   2 |   320/  696 batches | lr 30.00 | ms/batch 1786.71 | loss  6.15 | ppl   467.11\n",
      "| epoch   2 |   330/  696 batches | lr 30.00 | ms/batch 1868.14 | loss  6.09 | ppl   440.17\n",
      "| epoch   2 |   340/  696 batches | lr 30.00 | ms/batch 1864.28 | loss  6.11 | ppl   451.53\n",
      "| epoch   2 |   350/  696 batches | lr 30.00 | ms/batch 1781.20 | loss  6.10 | ppl   444.01\n",
      "| epoch   2 |   360/  696 batches | lr 30.00 | ms/batch 1802.81 | loss  6.04 | ppl   418.48\n",
      "| epoch   2 |   370/  696 batches | lr 30.00 | ms/batch 1803.41 | loss  6.10 | ppl   445.79\n",
      "| epoch   2 |   380/  696 batches | lr 30.00 | ms/batch 1811.99 | loss  6.08 | ppl   437.28\n",
      "| epoch   2 |   390/  696 batches | lr 30.00 | ms/batch 1798.21 | loss  6.02 | ppl   410.21\n",
      "| epoch   2 |   400/  696 batches | lr 30.00 | ms/batch 1792.42 | loss  6.05 | ppl   423.07\n",
      "| epoch   2 |   410/  696 batches | lr 30.00 | ms/batch 1811.56 | loss  6.08 | ppl   438.04\n",
      "| epoch   2 |   420/  696 batches | lr 30.00 | ms/batch 1796.78 | loss  6.03 | ppl   417.19\n",
      "| epoch   2 |   430/  696 batches | lr 30.00 | ms/batch 1801.48 | loss  6.02 | ppl   411.70\n",
      "| epoch   2 |   440/  696 batches | lr 30.00 | ms/batch 1786.23 | loss  6.01 | ppl   407.76\n",
      "| epoch   2 |   450/  696 batches | lr 30.00 | ms/batch 1818.97 | loss  6.05 | ppl   424.50\n",
      "| epoch   2 |   460/  696 batches | lr 30.00 | ms/batch 1801.04 | loss  6.01 | ppl   408.53\n",
      "| epoch   2 |   470/  696 batches | lr 30.00 | ms/batch 1793.44 | loss  6.05 | ppl   422.21\n",
      "| epoch   2 |   480/  696 batches | lr 30.00 | ms/batch 1788.24 | loss  6.01 | ppl   406.06\n",
      "| epoch   2 |   490/  696 batches | lr 30.00 | ms/batch 1832.74 | loss  6.04 | ppl   420.75\n",
      "| epoch   2 |   500/  696 batches | lr 30.00 | ms/batch 1782.61 | loss  5.98 | ppl   396.15\n",
      "| epoch   2 |   510/  696 batches | lr 30.00 | ms/batch 1821.54 | loss  5.98 | ppl   393.97\n",
      "| epoch   2 |   520/  696 batches | lr 30.00 | ms/batch 1806.12 | loss  5.97 | ppl   390.06\n",
      "| epoch   2 |   530/  696 batches | lr 30.00 | ms/batch 1815.26 | loss  5.96 | ppl   389.01\n",
      "| epoch   2 |   540/  696 batches | lr 30.00 | ms/batch 1799.79 | loss  5.99 | ppl   398.43\n",
      "| epoch   2 |   550/  696 batches | lr 30.00 | ms/batch 1810.89 | loss  5.98 | ppl   394.35\n",
      "| epoch   2 |   560/  696 batches | lr 30.00 | ms/batch 1811.62 | loss  5.97 | ppl   393.19\n",
      "| epoch   2 |   570/  696 batches | lr 30.00 | ms/batch 1797.93 | loss  5.95 | ppl   385.26\n",
      "| epoch   2 |   580/  696 batches | lr 30.00 | ms/batch 1795.71 | loss  5.95 | ppl   382.35\n",
      "| epoch   2 |   590/  696 batches | lr 30.00 | ms/batch 1798.31 | loss  6.01 | ppl   409.32\n",
      "| epoch   2 |   600/  696 batches | lr 30.00 | ms/batch 1809.23 | loss  5.98 | ppl   394.23\n",
      "| epoch   2 |   610/  696 batches | lr 30.00 | ms/batch 1799.84 | loss  5.90 | ppl   363.59\n",
      "| epoch   2 |   620/  696 batches | lr 30.00 | ms/batch 1800.30 | loss  5.98 | ppl   393.93\n",
      "| epoch   2 |   630/  696 batches | lr 30.00 | ms/batch 1796.57 | loss  5.95 | ppl   385.17\n",
      "| epoch   2 |   640/  696 batches | lr 30.00 | ms/batch 1807.03 | loss  5.94 | ppl   381.81\n",
      "| epoch   2 |   650/  696 batches | lr 30.00 | ms/batch 1797.31 | loss  5.94 | ppl   381.80\n",
      "| epoch   2 |   660/  696 batches | lr 30.00 | ms/batch 1801.21 | loss  5.97 | ppl   390.27\n",
      "| epoch   2 |   670/  696 batches | lr 30.00 | ms/batch 1801.22 | loss  5.96 | ppl   385.85\n",
      "| epoch   2 |   680/  696 batches | lr 30.00 | ms/batch 1807.92 | loss  5.93 | ppl   377.38\n",
      "| epoch   2 |   690/  696 batches | lr 30.00 | ms/batch 1810.30 | loss  5.88 | ppl   359.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1317.35s | valid loss  5.85 | valid ppl   345.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    10/  696 batches | lr 30.00 | ms/batch 1964.62 | loss  6.51 | ppl   674.76\n",
      "| epoch   3 |    20/  696 batches | lr 30.00 | ms/batch 1800.91 | loss  5.95 | ppl   382.02\n",
      "| epoch   3 |    30/  696 batches | lr 30.00 | ms/batch 1814.78 | loss  5.93 | ppl   376.38\n",
      "| epoch   3 |    40/  696 batches | lr 30.00 | ms/batch 1803.17 | loss  5.95 | ppl   385.57\n",
      "| epoch   3 |    50/  696 batches | lr 30.00 | ms/batch 1801.99 | loss  5.94 | ppl   380.61\n",
      "| epoch   3 |    60/  696 batches | lr 30.00 | ms/batch 1799.76 | loss  5.92 | ppl   371.38\n",
      "| epoch   3 |    70/  696 batches | lr 30.00 | ms/batch 1806.44 | loss  5.88 | ppl   358.46\n",
      "| epoch   3 |    80/  696 batches | lr 30.00 | ms/batch 1793.51 | loss  5.98 | ppl   394.10\n",
      "| epoch   3 |    90/  696 batches | lr 30.00 | ms/batch 1785.76 | loss  5.94 | ppl   380.24\n",
      "| epoch   3 |   100/  696 batches | lr 30.00 | ms/batch 1806.02 | loss  5.96 | ppl   389.35\n",
      "| epoch   3 |   110/  696 batches | lr 30.00 | ms/batch 1783.66 | loss  5.91 | ppl   369.95\n",
      "| epoch   3 |   120/  696 batches | lr 30.00 | ms/batch 1810.31 | loss  5.93 | ppl   374.31\n",
      "| epoch   3 |   130/  696 batches | lr 30.00 | ms/batch 1846.75 | loss  5.83 | ppl   340.80\n",
      "| epoch   3 |   140/  696 batches | lr 30.00 | ms/batch 1820.28 | loss  5.92 | ppl   370.87\n",
      "| epoch   3 |   150/  696 batches | lr 30.00 | ms/batch 1805.45 | loss  5.87 | ppl   352.96\n",
      "| epoch   3 |   160/  696 batches | lr 30.00 | ms/batch 1814.07 | loss  5.84 | ppl   343.27\n",
      "| epoch   3 |   170/  696 batches | lr 30.00 | ms/batch 1802.76 | loss  5.83 | ppl   340.57\n",
      "| epoch   3 |   180/  696 batches | lr 30.00 | ms/batch 1807.67 | loss  5.89 | ppl   361.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |   190/  696 batches | lr 30.00 | ms/batch 1813.06 | loss  5.92 | ppl   373.02\n",
      "| epoch   3 |   200/  696 batches | lr 30.00 | ms/batch 1789.75 | loss  5.91 | ppl   368.29\n",
      "| epoch   3 |   210/  696 batches | lr 30.00 | ms/batch 1804.42 | loss  5.88 | ppl   357.87\n",
      "| epoch   3 |   220/  696 batches | lr 30.00 | ms/batch 1820.85 | loss  5.85 | ppl   345.79\n",
      "| epoch   3 |   230/  696 batches | lr 30.00 | ms/batch 1798.36 | loss  5.94 | ppl   379.29\n",
      "| epoch   3 |   240/  696 batches | lr 30.00 | ms/batch 1789.07 | loss  5.87 | ppl   353.53\n",
      "| epoch   3 |   250/  696 batches | lr 30.00 | ms/batch 1814.45 | loss  5.87 | ppl   354.57\n",
      "| epoch   3 |   260/  696 batches | lr 30.00 | ms/batch 1813.24 | loss  5.80 | ppl   331.51\n",
      "| epoch   3 |   270/  696 batches | lr 30.00 | ms/batch 1786.96 | loss  5.88 | ppl   358.86\n",
      "| epoch   3 |   280/  696 batches | lr 30.00 | ms/batch 1796.88 | loss  5.83 | ppl   342.02\n",
      "| epoch   3 |   290/  696 batches | lr 30.00 | ms/batch 1836.86 | loss  5.89 | ppl   363.09\n",
      "| epoch   3 |   300/  696 batches | lr 30.00 | ms/batch 1822.05 | loss  5.86 | ppl   349.40\n",
      "| epoch   3 |   310/  696 batches | lr 30.00 | ms/batch 1794.79 | loss  5.82 | ppl   337.18\n",
      "| epoch   3 |   320/  696 batches | lr 30.00 | ms/batch 1796.33 | loss  5.83 | ppl   339.70\n",
      "| epoch   3 |   330/  696 batches | lr 30.00 | ms/batch 1821.83 | loss  5.84 | ppl   343.88\n",
      "| epoch   3 |   340/  696 batches | lr 30.00 | ms/batch 1787.51 | loss  5.85 | ppl   346.12\n",
      "| epoch   3 |   350/  696 batches | lr 30.00 | ms/batch 1794.47 | loss  5.83 | ppl   341.59\n",
      "| epoch   3 |   360/  696 batches | lr 30.00 | ms/batch 1815.69 | loss  5.80 | ppl   328.97\n",
      "| epoch   3 |   370/  696 batches | lr 30.00 | ms/batch 1797.47 | loss  5.84 | ppl   343.42\n",
      "| epoch   3 |   380/  696 batches | lr 30.00 | ms/batch 1780.36 | loss  5.78 | ppl   323.07\n",
      "| epoch   3 |   390/  696 batches | lr 30.00 | ms/batch 1788.24 | loss  5.79 | ppl   328.48\n",
      "| epoch   3 |   400/  696 batches | lr 30.00 | ms/batch 1831.08 | loss  5.80 | ppl   330.09\n",
      "| epoch   3 |   410/  696 batches | lr 30.00 | ms/batch 1799.06 | loss  5.85 | ppl   348.26\n",
      "| epoch   3 |   420/  696 batches | lr 30.00 | ms/batch 1792.16 | loss  5.78 | ppl   324.00\n",
      "| epoch   3 |   430/  696 batches | lr 30.00 | ms/batch 1805.73 | loss  5.79 | ppl   327.81\n",
      "| epoch   3 |   440/  696 batches | lr 30.00 | ms/batch 1917.99 | loss  5.75 | ppl   313.14\n",
      "| epoch   3 |   450/  696 batches | lr 30.00 | ms/batch 1796.53 | loss  5.84 | ppl   343.63\n",
      "| epoch   3 |   460/  696 batches | lr 30.00 | ms/batch 1796.75 | loss  5.77 | ppl   320.45\n",
      "| epoch   3 |   470/  696 batches | lr 30.00 | ms/batch 1720.13 | loss  5.80 | ppl   329.25\n",
      "| epoch   3 |   480/  696 batches | lr 30.00 | ms/batch 1457.66 | loss  5.77 | ppl   319.15\n",
      "| epoch   3 |   490/  696 batches | lr 30.00 | ms/batch 1380.10 | loss  5.80 | ppl   329.39\n",
      "| epoch   3 |   500/  696 batches | lr 30.00 | ms/batch 1403.61 | loss  5.75 | ppl   314.14\n",
      "| epoch   3 |   510/  696 batches | lr 30.00 | ms/batch 1425.60 | loss  5.72 | ppl   306.04\n",
      "| epoch   3 |   520/  696 batches | lr 30.00 | ms/batch 1403.17 | loss  5.77 | ppl   319.31\n",
      "| epoch   3 |   530/  696 batches | lr 30.00 | ms/batch 1416.37 | loss  5.71 | ppl   301.54\n",
      "| epoch   3 |   540/  696 batches | lr 30.00 | ms/batch 1426.24 | loss  5.77 | ppl   320.70\n",
      "| epoch   3 |   550/  696 batches | lr 30.00 | ms/batch 1415.40 | loss  5.76 | ppl   316.45\n",
      "| epoch   3 |   560/  696 batches | lr 30.00 | ms/batch 1422.72 | loss  5.76 | ppl   316.37\n",
      "| epoch   3 |   570/  696 batches | lr 30.00 | ms/batch 1416.72 | loss  5.77 | ppl   320.08\n",
      "| epoch   3 |   580/  696 batches | lr 30.00 | ms/batch 1448.25 | loss  5.73 | ppl   309.11\n",
      "| epoch   3 |   590/  696 batches | lr 30.00 | ms/batch 1441.95 | loss  5.77 | ppl   318.98\n",
      "| epoch   3 |   600/  696 batches | lr 30.00 | ms/batch 1417.83 | loss  5.75 | ppl   313.23\n",
      "| epoch   3 |   610/  696 batches | lr 30.00 | ms/batch 1432.97 | loss  5.67 | ppl   288.69\n",
      "| epoch   3 |   620/  696 batches | lr 30.00 | ms/batch 1444.29 | loss  5.74 | ppl   312.21\n",
      "| epoch   3 |   630/  696 batches | lr 30.00 | ms/batch 1421.13 | loss  5.75 | ppl   315.75\n",
      "| epoch   3 |   640/  696 batches | lr 30.00 | ms/batch 1422.78 | loss  5.73 | ppl   307.27\n",
      "| epoch   3 |   650/  696 batches | lr 30.00 | ms/batch 1435.49 | loss  5.76 | ppl   316.26\n",
      "| epoch   3 |   660/  696 batches | lr 30.00 | ms/batch 1444.45 | loss  5.73 | ppl   308.20\n",
      "| epoch   3 |   670/  696 batches | lr 30.00 | ms/batch 1453.11 | loss  5.73 | ppl   307.72\n",
      "| epoch   3 |   680/  696 batches | lr 30.00 | ms/batch 1412.82 | loss  5.74 | ppl   310.09\n",
      "| epoch   3 |   690/  696 batches | lr 30.00 | ms/batch 1439.02 | loss  5.69 | ppl   294.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1218.38s | valid loss  5.51 | valid ppl   246.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    10/  696 batches | lr 30.00 | ms/batch 1563.85 | loss  6.32 | ppl   553.01\n",
      "| epoch   4 |    20/  696 batches | lr 30.00 | ms/batch 1449.24 | loss  5.76 | ppl   316.24\n",
      "| epoch   4 |    30/  696 batches | lr 30.00 | ms/batch 1431.64 | loss  5.73 | ppl   307.74\n",
      "| epoch   4 |    40/  696 batches | lr 30.00 | ms/batch 1415.80 | loss  5.76 | ppl   315.81\n",
      "| epoch   4 |    50/  696 batches | lr 30.00 | ms/batch 1417.26 | loss  5.72 | ppl   305.89\n",
      "| epoch   4 |    60/  696 batches | lr 30.00 | ms/batch 1432.34 | loss  5.72 | ppl   306.26\n",
      "| epoch   4 |    70/  696 batches | lr 30.00 | ms/batch 1440.30 | loss  5.70 | ppl   300.11\n",
      "| epoch   4 |    80/  696 batches | lr 30.00 | ms/batch 1440.76 | loss  5.78 | ppl   324.56\n",
      "| epoch   4 |    90/  696 batches | lr 30.00 | ms/batch 1435.44 | loss  5.75 | ppl   313.03\n",
      "| epoch   4 |   100/  696 batches | lr 30.00 | ms/batch 1424.82 | loss  5.77 | ppl   320.35\n",
      "| epoch   4 |   110/  696 batches | lr 30.00 | ms/batch 1423.99 | loss  5.74 | ppl   311.34\n",
      "| epoch   4 |   120/  696 batches | lr 30.00 | ms/batch 1441.87 | loss  5.72 | ppl   304.01\n",
      "| epoch   4 |   130/  696 batches | lr 30.00 | ms/batch 1449.05 | loss  5.69 | ppl   297.37\n",
      "| epoch   4 |   140/  696 batches | lr 30.00 | ms/batch 1435.39 | loss  5.72 | ppl   306.32\n",
      "| epoch   4 |   150/  696 batches | lr 30.00 | ms/batch 1417.03 | loss  5.65 | ppl   285.65\n",
      "| epoch   4 |   160/  696 batches | lr 30.00 | ms/batch 1424.57 | loss  5.66 | ppl   286.97\n",
      "| epoch   4 |   170/  696 batches | lr 30.00 | ms/batch 1453.57 | loss  5.65 | ppl   283.28\n",
      "| epoch   4 |   180/  696 batches | lr 30.00 | ms/batch 1429.24 | loss  5.71 | ppl   300.38\n",
      "| epoch   4 |   190/  696 batches | lr 30.00 | ms/batch 1422.16 | loss  5.73 | ppl   308.26\n",
      "| epoch   4 |   200/  696 batches | lr 30.00 | ms/batch 1431.78 | loss  5.74 | ppl   310.69\n",
      "| epoch   4 |   210/  696 batches | lr 30.00 | ms/batch 1418.38 | loss  5.70 | ppl   298.63\n",
      "| epoch   4 |   220/  696 batches | lr 30.00 | ms/batch 1464.94 | loss  5.66 | ppl   287.44\n",
      "| epoch   4 |   230/  696 batches | lr 30.00 | ms/batch 1455.31 | loss  5.76 | ppl   317.81\n",
      "| epoch   4 |   240/  696 batches | lr 30.00 | ms/batch 1447.61 | loss  5.69 | ppl   297.05\n",
      "| epoch   4 |   250/  696 batches | lr 30.00 | ms/batch 1414.57 | loss  5.69 | ppl   296.83\n",
      "| epoch   4 |   260/  696 batches | lr 30.00 | ms/batch 1434.20 | loss  5.64 | ppl   282.07\n",
      "| epoch   4 |   270/  696 batches | lr 30.00 | ms/batch 1464.35 | loss  5.69 | ppl   295.23\n",
      "| epoch   4 |   280/  696 batches | lr 30.00 | ms/batch 1437.72 | loss  5.66 | ppl   288.40\n",
      "| epoch   4 |   290/  696 batches | lr 30.00 | ms/batch 1425.80 | loss  5.71 | ppl   302.10\n",
      "| epoch   4 |   300/  696 batches | lr 30.00 | ms/batch 1424.65 | loss  5.71 | ppl   301.20\n",
      "| epoch   4 |   310/  696 batches | lr 30.00 | ms/batch 1472.93 | loss  5.65 | ppl   284.04\n",
      "| epoch   4 |   320/  696 batches | lr 30.00 | ms/batch 1489.90 | loss  5.67 | ppl   291.16\n",
      "| epoch   4 |   330/  696 batches | lr 30.00 | ms/batch 1422.59 | loss  5.68 | ppl   292.30\n",
      "| epoch   4 |   340/  696 batches | lr 30.00 | ms/batch 1415.74 | loss  5.67 | ppl   291.03\n",
      "| epoch   4 |   350/  696 batches | lr 30.00 | ms/batch 1421.96 | loss  5.65 | ppl   283.35\n",
      "| epoch   4 |   360/  696 batches | lr 30.00 | ms/batch 1467.30 | loss  5.62 | ppl   275.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   370/  696 batches | lr 30.00 | ms/batch 1440.35 | loss  5.68 | ppl   291.96\n",
      "| epoch   4 |   380/  696 batches | lr 30.00 | ms/batch 1489.61 | loss  5.65 | ppl   282.95\n",
      "| epoch   4 |   390/  696 batches | lr 30.00 | ms/batch 1438.60 | loss  5.63 | ppl   279.16\n",
      "| epoch   4 |   400/  696 batches | lr 30.00 | ms/batch 1627.61 | loss  5.62 | ppl   274.92\n",
      "| epoch   4 |   410/  696 batches | lr 30.00 | ms/batch 1517.17 | loss  5.68 | ppl   292.44\n",
      "| epoch   4 |   420/  696 batches | lr 30.00 | ms/batch 1414.31 | loss  5.62 | ppl   274.76\n",
      "| epoch   4 |   430/  696 batches | lr 30.00 | ms/batch 1435.17 | loss  5.62 | ppl   275.88\n",
      "| epoch   4 |   440/  696 batches | lr 30.00 | ms/batch 1437.49 | loss  5.59 | ppl   268.83\n",
      "| epoch   4 |   450/  696 batches | lr 30.00 | ms/batch 1458.02 | loss  5.67 | ppl   291.25\n",
      "| epoch   4 |   460/  696 batches | lr 30.00 | ms/batch 1501.95 | loss  5.61 | ppl   272.87\n",
      "| epoch   4 |   470/  696 batches | lr 30.00 | ms/batch 1502.44 | loss  5.64 | ppl   282.66\n",
      "| epoch   4 |   480/  696 batches | lr 30.00 | ms/batch 1414.29 | loss  5.60 | ppl   270.60\n",
      "| epoch   4 |   490/  696 batches | lr 30.00 | ms/batch 1427.82 | loss  5.62 | ppl   275.59\n",
      "| epoch   4 |   500/  696 batches | lr 30.00 | ms/batch 1454.69 | loss  5.61 | ppl   273.10\n",
      "| epoch   4 |   510/  696 batches | lr 30.00 | ms/batch 1449.73 | loss  5.56 | ppl   260.23\n",
      "| epoch   4 |   520/  696 batches | lr 30.00 | ms/batch 1445.50 | loss  5.58 | ppl   265.37\n",
      "| epoch   4 |   530/  696 batches | lr 30.00 | ms/batch 1429.45 | loss  5.55 | ppl   257.17\n",
      "| epoch   4 |   540/  696 batches | lr 30.00 | ms/batch 1415.29 | loss  5.59 | ppl   268.04\n",
      "| epoch   4 |   550/  696 batches | lr 30.00 | ms/batch 1486.77 | loss  5.60 | ppl   270.64\n",
      "| epoch   4 |   560/  696 batches | lr 30.00 | ms/batch 1429.60 | loss  5.62 | ppl   275.14\n",
      "| epoch   4 |   570/  696 batches | lr 30.00 | ms/batch 1431.36 | loss  5.59 | ppl   268.50\n",
      "| epoch   4 |   580/  696 batches | lr 30.00 | ms/batch 1454.70 | loss  5.58 | ppl   266.37\n",
      "| epoch   4 |   590/  696 batches | lr 30.00 | ms/batch 1460.21 | loss  5.61 | ppl   273.97\n",
      "| epoch   4 |   600/  696 batches | lr 30.00 | ms/batch 1465.83 | loss  5.58 | ppl   265.86\n",
      "| epoch   4 |   610/  696 batches | lr 30.00 | ms/batch 1588.79 | loss  5.50 | ppl   245.50\n",
      "| epoch   4 |   620/  696 batches | lr 30.00 | ms/batch 1417.84 | loss  5.60 | ppl   270.00\n",
      "| epoch   4 |   630/  696 batches | lr 30.00 | ms/batch 1432.11 | loss  5.59 | ppl   268.31\n",
      "| epoch   4 |   640/  696 batches | lr 30.00 | ms/batch 1475.07 | loss  5.56 | ppl   258.88\n",
      "| epoch   4 |   650/  696 batches | lr 30.00 | ms/batch 1438.12 | loss  5.58 | ppl   264.50\n",
      "| epoch   4 |   660/  696 batches | lr 30.00 | ms/batch 1427.49 | loss  5.57 | ppl   261.87\n",
      "| epoch   4 |   670/  696 batches | lr 30.00 | ms/batch 1418.29 | loss  5.58 | ppl   266.30\n",
      "| epoch   4 |   680/  696 batches | lr 30.00 | ms/batch 1429.90 | loss  5.58 | ppl   264.20\n",
      "| epoch   4 |   690/  696 batches | lr 30.00 | ms/batch 1443.11 | loss  5.54 | ppl   254.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 1056.57s | valid loss  5.47 | valid ppl   237.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    10/  696 batches | lr 30.00 | ms/batch 1605.49 | loss  6.17 | ppl   480.16\n",
      "| epoch   5 |    20/  696 batches | lr 30.00 | ms/batch 1454.08 | loss  5.61 | ppl   273.48\n",
      "| epoch   5 |    30/  696 batches | lr 30.00 | ms/batch 1478.45 | loss  5.60 | ppl   270.35\n",
      "| epoch   5 |    40/  696 batches | lr 30.00 | ms/batch 1444.62 | loss  5.64 | ppl   281.23\n",
      "| epoch   5 |    50/  696 batches | lr 30.00 | ms/batch 1449.27 | loss  5.60 | ppl   269.53\n",
      "| epoch   5 |    60/  696 batches | lr 30.00 | ms/batch 1442.85 | loss  5.58 | ppl   264.99\n",
      "| epoch   5 |    70/  696 batches | lr 30.00 | ms/batch 1424.01 | loss  5.56 | ppl   259.66\n",
      "| epoch   5 |    80/  696 batches | lr 30.00 | ms/batch 1437.11 | loss  5.64 | ppl   281.18\n",
      "| epoch   5 |    90/  696 batches | lr 30.00 | ms/batch 1445.91 | loss  5.61 | ppl   271.83\n",
      "| epoch   5 |   100/  696 batches | lr 30.00 | ms/batch 1478.22 | loss  5.63 | ppl   279.35\n",
      "| epoch   5 |   110/  696 batches | lr 30.00 | ms/batch 1437.64 | loss  5.60 | ppl   269.68\n",
      "| epoch   5 |   120/  696 batches | lr 30.00 | ms/batch 1493.12 | loss  5.59 | ppl   267.19\n",
      "| epoch   5 |   130/  696 batches | lr 30.00 | ms/batch 1421.98 | loss  5.54 | ppl   254.89\n",
      "| epoch   5 |   140/  696 batches | lr 30.00 | ms/batch 1449.03 | loss  5.58 | ppl   264.81\n",
      "| epoch   5 |   150/  696 batches | lr 30.00 | ms/batch 1442.18 | loss  5.54 | ppl   254.70\n",
      "| epoch   5 |   160/  696 batches | lr 30.00 | ms/batch 1468.38 | loss  5.54 | ppl   255.48\n",
      "| epoch   5 |   170/  696 batches | lr 30.00 | ms/batch 1433.04 | loss  5.51 | ppl   246.05\n",
      "| epoch   5 |   180/  696 batches | lr 30.00 | ms/batch 1411.90 | loss  5.57 | ppl   262.87\n",
      "| epoch   5 |   190/  696 batches | lr 30.00 | ms/batch 1500.78 | loss  5.59 | ppl   268.12\n",
      "| epoch   5 |   200/  696 batches | lr 30.00 | ms/batch 1442.34 | loss  5.61 | ppl   272.33\n",
      "| epoch   5 |   210/  696 batches | lr 30.00 | ms/batch 1431.24 | loss  5.58 | ppl   264.87\n",
      "| epoch   5 |   220/  696 batches | lr 30.00 | ms/batch 1442.59 | loss  5.54 | ppl   253.44\n",
      "| epoch   5 |   230/  696 batches | lr 30.00 | ms/batch 1478.54 | loss  5.63 | ppl   279.23\n",
      "| epoch   5 |   240/  696 batches | lr 30.00 | ms/batch 1511.02 | loss  5.56 | ppl   260.86\n",
      "| epoch   5 |   250/  696 batches | lr 30.00 | ms/batch 1429.19 | loss  5.55 | ppl   258.49\n",
      "| epoch   5 |   260/  696 batches | lr 30.00 | ms/batch 1429.44 | loss  5.51 | ppl   248.19\n",
      "| epoch   5 |   270/  696 batches | lr 30.00 | ms/batch 1444.30 | loss  5.59 | ppl   266.60\n",
      "| epoch   5 |   280/  696 batches | lr 30.00 | ms/batch 1437.42 | loss  5.55 | ppl   257.94\n",
      "| epoch   5 |   290/  696 batches | lr 30.00 | ms/batch 1452.62 | loss  5.59 | ppl   267.53\n",
      "| epoch   5 |   300/  696 batches | lr 30.00 | ms/batch 1455.53 | loss  5.58 | ppl   265.93\n",
      "| epoch   5 |   310/  696 batches | lr 30.00 | ms/batch 1434.31 | loss  5.52 | ppl   249.87\n",
      "| epoch   5 |   320/  696 batches | lr 30.00 | ms/batch 1445.80 | loss  5.59 | ppl   266.45\n",
      "| epoch   5 |   330/  696 batches | lr 30.00 | ms/batch 1429.15 | loss  5.55 | ppl   258.29\n",
      "| epoch   5 |   340/  696 batches | lr 30.00 | ms/batch 1429.08 | loss  5.55 | ppl   256.16\n",
      "| epoch   5 |   350/  696 batches | lr 30.00 | ms/batch 1486.97 | loss  5.52 | ppl   250.30\n",
      "| epoch   5 |   360/  696 batches | lr 30.00 | ms/batch 1433.49 | loss  5.52 | ppl   249.96\n",
      "| epoch   5 |   370/  696 batches | lr 30.00 | ms/batch 1432.68 | loss  5.56 | ppl   258.68\n",
      "| epoch   5 |   380/  696 batches | lr 30.00 | ms/batch 1451.85 | loss  5.51 | ppl   247.01\n",
      "| epoch   5 |   390/  696 batches | lr 30.00 | ms/batch 1428.57 | loss  5.50 | ppl   245.48\n",
      "| epoch   5 |   400/  696 batches | lr 30.00 | ms/batch 1425.98 | loss  5.52 | ppl   250.72\n",
      "| epoch   5 |   410/  696 batches | lr 30.00 | ms/batch 1475.48 | loss  5.55 | ppl   258.50\n",
      "| epoch   5 |   420/  696 batches | lr 30.00 | ms/batch 1433.90 | loss  5.50 | ppl   244.87\n",
      "| epoch   5 |   430/  696 batches | lr 30.00 | ms/batch 1463.70 | loss  5.51 | ppl   246.16\n",
      "| epoch   5 |   440/  696 batches | lr 30.00 | ms/batch 1432.23 | loss  5.47 | ppl   237.26\n",
      "| epoch   5 |   450/  696 batches | lr 30.00 | ms/batch 1432.18 | loss  5.55 | ppl   256.80\n",
      "| epoch   5 |   460/  696 batches | lr 30.00 | ms/batch 1423.40 | loss  5.49 | ppl   242.86\n",
      "| epoch   5 |   470/  696 batches | lr 30.00 | ms/batch 1453.31 | loss  5.52 | ppl   249.88\n",
      "| epoch   5 |   480/  696 batches | lr 30.00 | ms/batch 1499.86 | loss  5.51 | ppl   245.95\n",
      "| epoch   5 |   490/  696 batches | lr 30.00 | ms/batch 1417.91 | loss  5.49 | ppl   241.33\n",
      "| epoch   5 |   500/  696 batches | lr 30.00 | ms/batch 1437.62 | loss  5.50 | ppl   244.44\n",
      "| epoch   5 |   510/  696 batches | lr 30.00 | ms/batch 1441.01 | loss  5.43 | ppl   227.07\n",
      "| epoch   5 |   520/  696 batches | lr 30.00 | ms/batch 1562.72 | loss  5.47 | ppl   238.30\n",
      "| epoch   5 |   530/  696 batches | lr 30.00 | ms/batch 1436.81 | loss  5.41 | ppl   224.29\n",
      "| epoch   5 |   540/  696 batches | lr 30.00 | ms/batch 1469.04 | loss  5.46 | ppl   235.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |   550/  696 batches | lr 30.00 | ms/batch 1435.01 | loss  5.48 | ppl   238.82\n",
      "| epoch   5 |   560/  696 batches | lr 30.00 | ms/batch 1506.47 | loss  5.51 | ppl   246.85\n",
      "| epoch   5 |   570/  696 batches | lr 30.00 | ms/batch 1435.58 | loss  5.49 | ppl   241.58\n",
      "| epoch   5 |   580/  696 batches | lr 30.00 | ms/batch 1440.36 | loss  5.47 | ppl   238.49\n",
      "| epoch   5 |   590/  696 batches | lr 30.00 | ms/batch 1438.27 | loss  5.49 | ppl   241.30\n",
      "| epoch   5 |   600/  696 batches | lr 30.00 | ms/batch 1417.74 | loss  5.47 | ppl   238.09\n",
      "| epoch   5 |   610/  696 batches | lr 30.00 | ms/batch 1458.83 | loss  5.39 | ppl   220.03\n",
      "| epoch   5 |   620/  696 batches | lr 30.00 | ms/batch 1448.79 | loss  5.48 | ppl   239.73\n",
      "| epoch   5 |   630/  696 batches | lr 30.00 | ms/batch 1434.98 | loss  5.49 | ppl   243.19\n",
      "| epoch   5 |   640/  696 batches | lr 30.00 | ms/batch 1445.44 | loss  5.45 | ppl   233.06\n",
      "| epoch   5 |   650/  696 batches | lr 30.00 | ms/batch 1449.84 | loss  5.48 | ppl   238.90\n",
      "| epoch   5 |   660/  696 batches | lr 30.00 | ms/batch 1455.90 | loss  5.47 | ppl   237.29\n",
      "| epoch   5 |   670/  696 batches | lr 30.00 | ms/batch 1444.46 | loss  5.46 | ppl   236.18\n",
      "| epoch   5 |   680/  696 batches | lr 30.00 | ms/batch 1430.77 | loss  5.48 | ppl   238.82\n",
      "| epoch   5 |   690/  696 batches | lr 30.00 | ms/batch 1423.38 | loss  5.43 | ppl   228.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 1056.73s | valid loss  5.38 | valid ppl   216.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    10/  696 batches | lr 30.00 | ms/batch 1594.39 | loss  6.07 | ppl   434.24\n",
      "| epoch   6 |    20/  696 batches | lr 30.00 | ms/batch 1460.19 | loss  5.51 | ppl   246.62\n",
      "| epoch   6 |    30/  696 batches | lr 30.00 | ms/batch 1423.47 | loss  5.49 | ppl   242.44\n",
      "| epoch   6 |    40/  696 batches | lr 30.00 | ms/batch 1430.30 | loss  5.53 | ppl   253.37\n",
      "| epoch   6 |    50/  696 batches | lr 30.00 | ms/batch 1482.43 | loss  5.48 | ppl   239.33\n",
      "| epoch   6 |    60/  696 batches | lr 30.00 | ms/batch 1472.98 | loss  5.48 | ppl   239.27\n",
      "| epoch   6 |    70/  696 batches | lr 30.00 | ms/batch 1651.48 | loss  5.45 | ppl   232.98\n",
      "| epoch   6 |    80/  696 batches | lr 30.00 | ms/batch 1553.27 | loss  5.53 | ppl   252.71\n",
      "| epoch   6 |    90/  696 batches | lr 30.00 | ms/batch 1458.46 | loss  5.51 | ppl   246.53\n",
      "| epoch   6 |   100/  696 batches | lr 30.00 | ms/batch 1451.57 | loss  5.54 | ppl   254.25\n",
      "| epoch   6 |   110/  696 batches | lr 30.00 | ms/batch 1467.89 | loss  5.51 | ppl   246.68\n",
      "| epoch   6 |   120/  696 batches | lr 30.00 | ms/batch 1528.12 | loss  5.48 | ppl   238.87\n",
      "| epoch   6 |   130/  696 batches | lr 30.00 | ms/batch 1580.04 | loss  5.43 | ppl   228.35\n",
      "| epoch   6 |   140/  696 batches | lr 30.00 | ms/batch 1816.92 | loss  5.50 | ppl   244.52\n",
      "| epoch   6 |   150/  696 batches | lr 30.00 | ms/batch 1508.21 | loss  5.43 | ppl   227.74\n",
      "| epoch   6 |   160/  696 batches | lr 30.00 | ms/batch 1532.29 | loss  5.43 | ppl   228.56\n",
      "| epoch   6 |   170/  696 batches | lr 30.00 | ms/batch 1482.91 | loss  5.40 | ppl   221.85\n",
      "| epoch   6 |   180/  696 batches | lr 30.00 | ms/batch 1559.62 | loss  5.46 | ppl   235.74\n",
      "| epoch   6 |   190/  696 batches | lr 30.00 | ms/batch 1512.90 | loss  5.51 | ppl   246.71\n",
      "| epoch   6 |   200/  696 batches | lr 30.00 | ms/batch 1494.79 | loss  5.52 | ppl   248.67\n",
      "| epoch   6 |   210/  696 batches | lr 30.00 | ms/batch 1508.51 | loss  5.48 | ppl   240.60\n",
      "| epoch   6 |   220/  696 batches | lr 30.00 | ms/batch 1467.74 | loss  5.46 | ppl   235.99\n",
      "| epoch   6 |   230/  696 batches | lr 30.00 | ms/batch 1509.98 | loss  5.52 | ppl   249.00\n",
      "| epoch   6 |   240/  696 batches | lr 30.00 | ms/batch 1505.15 | loss  5.47 | ppl   238.14\n",
      "| epoch   6 |   250/  696 batches | lr 30.00 | ms/batch 1471.95 | loss  5.46 | ppl   234.53\n",
      "| epoch   6 |   260/  696 batches | lr 30.00 | ms/batch 1474.07 | loss  5.43 | ppl   228.16\n",
      "| epoch   6 |   270/  696 batches | lr 30.00 | ms/batch 1490.02 | loss  5.50 | ppl   244.06\n",
      "| epoch   6 |   280/  696 batches | lr 30.00 | ms/batch 1495.48 | loss  5.44 | ppl   231.39\n",
      "| epoch   6 |   290/  696 batches | lr 30.00 | ms/batch 1489.26 | loss  5.51 | ppl   246.74\n",
      "| epoch   6 |   300/  696 batches | lr 30.00 | ms/batch 1487.93 | loss  5.50 | ppl   245.30\n",
      "| epoch   6 |   310/  696 batches | lr 30.00 | ms/batch 1468.15 | loss  5.43 | ppl   228.12\n",
      "| epoch   6 |   320/  696 batches | lr 30.00 | ms/batch 1533.45 | loss  5.48 | ppl   239.03\n",
      "| epoch   6 |   330/  696 batches | lr 30.00 | ms/batch 1474.08 | loss  5.46 | ppl   234.93\n",
      "| epoch   6 |   340/  696 batches | lr 30.00 | ms/batch 1580.12 | loss  5.44 | ppl   231.28\n",
      "| epoch   6 |   350/  696 batches | lr 30.00 | ms/batch 1479.18 | loss  5.44 | ppl   230.01\n",
      "| epoch   6 |   360/  696 batches | lr 30.00 | ms/batch 1505.82 | loss  5.42 | ppl   225.32\n",
      "| epoch   6 |   370/  696 batches | lr 30.00 | ms/batch 1478.66 | loss  5.45 | ppl   232.15\n",
      "| epoch   6 |   380/  696 batches | lr 30.00 | ms/batch 1540.10 | loss  5.42 | ppl   225.12\n",
      "| epoch   6 |   390/  696 batches | lr 30.00 | ms/batch 1523.71 | loss  5.40 | ppl   222.46\n",
      "| epoch   6 |   400/  696 batches | lr 30.00 | ms/batch 1462.30 | loss  5.43 | ppl   227.85\n",
      "| epoch   6 |   410/  696 batches | lr 30.00 | ms/batch 1496.79 | loss  5.45 | ppl   233.86\n",
      "| epoch   6 |   420/  696 batches | lr 30.00 | ms/batch 1479.90 | loss  5.42 | ppl   226.56\n",
      "| epoch   6 |   430/  696 batches | lr 30.00 | ms/batch 1467.04 | loss  5.41 | ppl   223.33\n",
      "| epoch   6 |   440/  696 batches | lr 30.00 | ms/batch 1477.57 | loss  5.37 | ppl   215.47\n",
      "| epoch   6 |   450/  696 batches | lr 30.00 | ms/batch 1491.73 | loss  5.47 | ppl   237.28\n",
      "| epoch   6 |   460/  696 batches | lr 30.00 | ms/batch 1466.63 | loss  5.39 | ppl   218.16\n",
      "| epoch   6 |   470/  696 batches | lr 30.00 | ms/batch 1471.86 | loss  5.43 | ppl   228.89\n",
      "| epoch   6 |   480/  696 batches | lr 30.00 | ms/batch 1482.38 | loss  5.41 | ppl   223.47\n",
      "| epoch   6 |   490/  696 batches | lr 30.00 | ms/batch 1486.22 | loss  5.41 | ppl   222.97\n",
      "| epoch   6 |   500/  696 batches | lr 30.00 | ms/batch 1487.66 | loss  5.41 | ppl   224.02\n",
      "| epoch   6 |   510/  696 batches | lr 30.00 | ms/batch 1465.60 | loss  5.35 | ppl   210.00\n",
      "| epoch   6 |   520/  696 batches | lr 30.00 | ms/batch 1459.31 | loss  5.37 | ppl   215.67\n",
      "| epoch   6 |   530/  696 batches | lr 30.00 | ms/batch 1503.00 | loss  5.33 | ppl   206.69\n",
      "| epoch   6 |   540/  696 batches | lr 30.00 | ms/batch 1561.60 | loss  5.37 | ppl   215.84\n",
      "| epoch   6 |   550/  696 batches | lr 30.00 | ms/batch 1473.29 | loss  5.39 | ppl   220.24\n",
      "| epoch   6 |   560/  696 batches | lr 30.00 | ms/batch 1460.24 | loss  5.41 | ppl   224.12\n",
      "| epoch   6 |   570/  696 batches | lr 30.00 | ms/batch 1497.54 | loss  5.39 | ppl   218.65\n",
      "| epoch   6 |   580/  696 batches | lr 30.00 | ms/batch 1503.54 | loss  5.38 | ppl   218.10\n",
      "| epoch   6 |   590/  696 batches | lr 30.00 | ms/batch 1482.27 | loss  5.41 | ppl   223.82\n",
      "| epoch   6 |   600/  696 batches | lr 30.00 | ms/batch 1478.54 | loss  5.39 | ppl   218.96\n",
      "| epoch   6 |   610/  696 batches | lr 30.00 | ms/batch 1484.78 | loss  5.30 | ppl   200.97\n",
      "| epoch   6 |   620/  696 batches | lr 30.00 | ms/batch 1509.27 | loss  5.38 | ppl   216.49\n",
      "| epoch   6 |   630/  696 batches | lr 30.00 | ms/batch 1538.21 | loss  5.40 | ppl   222.12\n",
      "| epoch   6 |   640/  696 batches | lr 30.00 | ms/batch 1511.81 | loss  5.38 | ppl   216.92\n",
      "| epoch   6 |   650/  696 batches | lr 30.00 | ms/batch 1497.80 | loss  5.39 | ppl   218.84\n",
      "| epoch   6 |   660/  696 batches | lr 30.00 | ms/batch 1466.80 | loss  5.38 | ppl   217.35\n",
      "| epoch   6 |   670/  696 batches | lr 30.00 | ms/batch 1507.51 | loss  5.39 | ppl   218.98\n",
      "| epoch   6 |   680/  696 batches | lr 30.00 | ms/batch 1506.09 | loss  5.40 | ppl   221.05\n",
      "| epoch   6 |   690/  696 batches | lr 30.00 | ms/batch 1524.58 | loss  5.34 | ppl   209.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 1095.68s | valid loss  5.31 | valid ppl   203.06\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |    10/  696 batches | lr 30.00 | ms/batch 1641.81 | loss  6.00 | ppl   404.63\n",
      "| epoch   7 |    20/  696 batches | lr 30.00 | ms/batch 1530.21 | loss  5.43 | ppl   227.93\n",
      "| epoch   7 |    30/  696 batches | lr 30.00 | ms/batch 1457.14 | loss  5.41 | ppl   223.98\n",
      "| epoch   7 |    40/  696 batches | lr 30.00 | ms/batch 1486.60 | loss  5.46 | ppl   234.64\n",
      "| epoch   7 |    50/  696 batches | lr 30.00 | ms/batch 1502.50 | loss  5.40 | ppl   221.22\n",
      "| epoch   7 |    60/  696 batches | lr 30.00 | ms/batch 1467.79 | loss  5.39 | ppl   220.04\n",
      "| epoch   7 |    70/  696 batches | lr 30.00 | ms/batch 1514.75 | loss  5.37 | ppl   214.13\n",
      "| epoch   7 |    80/  696 batches | lr 30.00 | ms/batch 1468.08 | loss  5.45 | ppl   232.13\n",
      "| epoch   7 |    90/  696 batches | lr 30.00 | ms/batch 1476.05 | loss  5.43 | ppl   228.61\n",
      "| epoch   7 |   100/  696 batches | lr 30.00 | ms/batch 1542.29 | loss  5.46 | ppl   234.64\n",
      "| epoch   7 |   110/  696 batches | lr 30.00 | ms/batch 1493.10 | loss  5.42 | ppl   226.94\n",
      "| epoch   7 |   120/  696 batches | lr 30.00 | ms/batch 1492.73 | loss  5.40 | ppl   221.18\n",
      "| epoch   7 |   130/  696 batches | lr 30.00 | ms/batch 1457.88 | loss  5.36 | ppl   213.42\n",
      "| epoch   7 |   140/  696 batches | lr 30.00 | ms/batch 1526.33 | loss  5.43 | ppl   228.77\n",
      "| epoch   7 |   150/  696 batches | lr 30.00 | ms/batch 1550.14 | loss  5.36 | ppl   212.46\n",
      "| epoch   7 |   160/  696 batches | lr 30.00 | ms/batch 1525.08 | loss  5.36 | ppl   213.03\n",
      "| epoch   7 |   170/  696 batches | lr 30.00 | ms/batch 1514.18 | loss  5.34 | ppl   207.83\n",
      "| epoch   7 |   180/  696 batches | lr 30.00 | ms/batch 1483.09 | loss  5.42 | ppl   225.81\n",
      "| epoch   7 |   190/  696 batches | lr 30.00 | ms/batch 1482.97 | loss  5.42 | ppl   225.18\n",
      "| epoch   7 |   200/  696 batches | lr 30.00 | ms/batch 1505.37 | loss  5.42 | ppl   224.94\n",
      "| epoch   7 |   210/  696 batches | lr 30.00 | ms/batch 1578.14 | loss  5.41 | ppl   223.51\n",
      "| epoch   7 |   220/  696 batches | lr 30.00 | ms/batch 1493.29 | loss  5.37 | ppl   215.33\n",
      "| epoch   7 |   230/  696 batches | lr 30.00 | ms/batch 1452.36 | loss  5.43 | ppl   228.87\n",
      "| epoch   7 |   240/  696 batches | lr 30.00 | ms/batch 1502.32 | loss  5.40 | ppl   222.28\n",
      "| epoch   7 |   250/  696 batches | lr 30.00 | ms/batch 1526.58 | loss  5.37 | ppl   215.42\n",
      "| epoch   7 |   260/  696 batches | lr 30.00 | ms/batch 1486.61 | loss  5.35 | ppl   209.62\n",
      "| epoch   7 |   270/  696 batches | lr 30.00 | ms/batch 1486.37 | loss  5.43 | ppl   227.57\n",
      "| epoch   7 |   280/  696 batches | lr 30.00 | ms/batch 1519.47 | loss  5.37 | ppl   215.38\n",
      "| epoch   7 |   290/  696 batches | lr 30.00 | ms/batch 1497.39 | loss  5.45 | ppl   232.76\n",
      "| epoch   7 |   300/  696 batches | lr 30.00 | ms/batch 1541.43 | loss  5.43 | ppl   227.87\n",
      "| epoch   7 |   310/  696 batches | lr 30.00 | ms/batch 1513.14 | loss  5.37 | ppl   214.85\n",
      "| epoch   7 |   320/  696 batches | lr 30.00 | ms/batch 1506.49 | loss  5.41 | ppl   223.87\n",
      "| epoch   7 |   330/  696 batches | lr 30.00 | ms/batch 1543.90 | loss  5.39 | ppl   218.42\n",
      "| epoch   7 |   340/  696 batches | lr 30.00 | ms/batch 1455.21 | loss  5.37 | ppl   214.10\n",
      "| epoch   7 |   350/  696 batches | lr 30.00 | ms/batch 1514.74 | loss  5.37 | ppl   215.82\n",
      "| epoch   7 |   360/  696 batches | lr 30.00 | ms/batch 1522.02 | loss  5.35 | ppl   209.72\n",
      "| epoch   7 |   370/  696 batches | lr 30.00 | ms/batch 1523.44 | loss  5.40 | ppl   220.82\n",
      "| epoch   7 |   380/  696 batches | lr 30.00 | ms/batch 1485.91 | loss  5.35 | ppl   210.32\n",
      "| epoch   7 |   390/  696 batches | lr 30.00 | ms/batch 1514.19 | loss  5.33 | ppl   206.73\n",
      "| epoch   7 |   400/  696 batches | lr 30.00 | ms/batch 1483.37 | loss  5.35 | ppl   211.28\n",
      "| epoch   7 |   410/  696 batches | lr 30.00 | ms/batch 1483.40 | loss  5.39 | ppl   220.26\n",
      "| epoch   7 |   420/  696 batches | lr 30.00 | ms/batch 1504.02 | loss  5.33 | ppl   206.97\n",
      "| epoch   7 |   430/  696 batches | lr 30.00 | ms/batch 1489.09 | loss  5.37 | ppl   214.47\n",
      "| epoch   7 |   440/  696 batches | lr 30.00 | ms/batch 1508.27 | loss  5.32 | ppl   203.80\n",
      "| epoch   7 |   450/  696 batches | lr 30.00 | ms/batch 1516.54 | loss  5.38 | ppl   217.14\n",
      "| epoch   7 |   460/  696 batches | lr 30.00 | ms/batch 1516.92 | loss  5.35 | ppl   211.20\n",
      "| epoch   7 |   470/  696 batches | lr 30.00 | ms/batch 1493.52 | loss  5.36 | ppl   211.74\n",
      "| epoch   7 |   480/  696 batches | lr 30.00 | ms/batch 1477.14 | loss  5.33 | ppl   206.03\n",
      "| epoch   7 |   490/  696 batches | lr 30.00 | ms/batch 1482.01 | loss  5.35 | ppl   209.85\n",
      "| epoch   7 |   500/  696 batches | lr 30.00 | ms/batch 1550.57 | loss  5.34 | ppl   207.98\n",
      "| epoch   7 |   510/  696 batches | lr 30.00 | ms/batch 1555.93 | loss  5.28 | ppl   197.00\n",
      "| epoch   7 |   520/  696 batches | lr 30.00 | ms/batch 1477.33 | loss  5.32 | ppl   204.59\n",
      "| epoch   7 |   530/  696 batches | lr 30.00 | ms/batch 1502.24 | loss  5.26 | ppl   192.91\n",
      "| epoch   7 |   540/  696 batches | lr 30.00 | ms/batch 1498.25 | loss  5.31 | ppl   201.70\n",
      "| epoch   7 |   550/  696 batches | lr 30.00 | ms/batch 1492.77 | loss  5.33 | ppl   205.73\n",
      "| epoch   7 |   560/  696 batches | lr 30.00 | ms/batch 1497.68 | loss  5.35 | ppl   211.59\n",
      "| epoch   7 |   570/  696 batches | lr 30.00 | ms/batch 1558.24 | loss  5.32 | ppl   204.88\n",
      "| epoch   7 |   580/  696 batches | lr 30.00 | ms/batch 1504.95 | loss  5.32 | ppl   204.01\n",
      "| epoch   7 |   590/  696 batches | lr 30.00 | ms/batch 1556.81 | loss  5.33 | ppl   206.25\n",
      "| epoch   7 |   600/  696 batches | lr 30.00 | ms/batch 1467.15 | loss  5.31 | ppl   202.37\n",
      "| epoch   7 |   610/  696 batches | lr 30.00 | ms/batch 1472.66 | loss  5.24 | ppl   189.37\n",
      "| epoch   7 |   620/  696 batches | lr 30.00 | ms/batch 1516.75 | loss  5.31 | ppl   202.83\n",
      "| epoch   7 |   630/  696 batches | lr 30.00 | ms/batch 1503.73 | loss  5.34 | ppl   208.17\n",
      "| epoch   7 |   640/  696 batches | lr 30.00 | ms/batch 1483.04 | loss  5.30 | ppl   200.46\n",
      "| epoch   7 |   650/  696 batches | lr 30.00 | ms/batch 1547.20 | loss  5.29 | ppl   197.42\n",
      "| epoch   7 |   660/  696 batches | lr 30.00 | ms/batch 1500.67 | loss  5.34 | ppl   209.49\n",
      "| epoch   7 |   670/  696 batches | lr 30.00 | ms/batch 1508.05 | loss  5.34 | ppl   207.51\n",
      "| epoch   7 |   680/  696 batches | lr 30.00 | ms/batch 1492.01 | loss  5.31 | ppl   202.26\n",
      "| epoch   7 |   690/  696 batches | lr 30.00 | ms/batch 1489.28 | loss  5.28 | ppl   196.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 1096.16s | valid loss  5.18 | valid ppl   178.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    10/  696 batches | lr 30.00 | ms/batch 1722.21 | loss  5.92 | ppl   371.06\n",
      "| epoch   8 |    20/  696 batches | lr 30.00 | ms/batch 1535.47 | loss  5.38 | ppl   216.10\n",
      "| epoch   8 |    30/  696 batches | lr 30.00 | ms/batch 1547.46 | loss  5.35 | ppl   210.05\n",
      "| epoch   8 |    40/  696 batches | lr 30.00 | ms/batch 1512.02 | loss  5.38 | ppl   217.40\n",
      "| epoch   8 |    50/  696 batches | lr 30.00 | ms/batch 1503.98 | loss  5.34 | ppl   209.45\n",
      "| epoch   8 |    60/  696 batches | lr 30.00 | ms/batch 1502.49 | loss  5.32 | ppl   205.19\n",
      "| epoch   8 |    70/  696 batches | lr 30.00 | ms/batch 1509.26 | loss  5.32 | ppl   203.93\n",
      "| epoch   8 |    80/  696 batches | lr 30.00 | ms/batch 1588.76 | loss  5.39 | ppl   218.21\n",
      "| epoch   8 |    90/  696 batches | lr 30.00 | ms/batch 1459.53 | loss  5.36 | ppl   211.93\n",
      "| epoch   8 |   100/  696 batches | lr 30.00 | ms/batch 1508.74 | loss  5.40 | ppl   220.54\n",
      "| epoch   8 |   110/  696 batches | lr 30.00 | ms/batch 1501.57 | loss  5.36 | ppl   213.27\n",
      "| epoch   8 |   120/  696 batches | lr 30.00 | ms/batch 1473.08 | loss  5.35 | ppl   210.14\n",
      "| epoch   8 |   130/  696 batches | lr 30.00 | ms/batch 1475.24 | loss  5.32 | ppl   204.34\n",
      "| epoch   8 |   140/  696 batches | lr 30.00 | ms/batch 1491.04 | loss  5.36 | ppl   212.17\n",
      "| epoch   8 |   150/  696 batches | lr 30.00 | ms/batch 1486.99 | loss  5.28 | ppl   196.26\n",
      "| epoch   8 |   160/  696 batches | lr 30.00 | ms/batch 1525.15 | loss  5.29 | ppl   197.81\n",
      "| epoch   8 |   170/  696 batches | lr 30.00 | ms/batch 1529.42 | loss  5.26 | ppl   191.59\n",
      "| epoch   8 |   180/  696 batches | lr 30.00 | ms/batch 1515.89 | loss  5.34 | ppl   209.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |   190/  696 batches | lr 30.00 | ms/batch 1470.94 | loss  5.36 | ppl   212.33\n",
      "| epoch   8 |   200/  696 batches | lr 30.00 | ms/batch 1529.78 | loss  5.37 | ppl   214.23\n",
      "| epoch   8 |   210/  696 batches | lr 30.00 | ms/batch 1500.65 | loss  5.35 | ppl   211.25\n",
      "| epoch   8 |   220/  696 batches | lr 30.00 | ms/batch 1503.32 | loss  5.32 | ppl   205.15\n",
      "| epoch   8 |   230/  696 batches | lr 30.00 | ms/batch 1488.79 | loss  5.39 | ppl   218.50\n",
      "| epoch   8 |   240/  696 batches | lr 30.00 | ms/batch 1496.00 | loss  5.34 | ppl   207.60\n",
      "| epoch   8 |   250/  696 batches | lr 30.00 | ms/batch 1506.65 | loss  5.33 | ppl   205.98\n",
      "| epoch   8 |   260/  696 batches | lr 30.00 | ms/batch 1636.54 | loss  5.29 | ppl   198.04\n",
      "| epoch   8 |   270/  696 batches | lr 30.00 | ms/batch 1445.35 | loss  5.38 | ppl   217.56\n",
      "| epoch   8 |   280/  696 batches | lr 30.00 | ms/batch 1452.19 | loss  5.31 | ppl   203.09\n",
      "| epoch   8 |   290/  696 batches | lr 30.00 | ms/batch 1469.00 | loss  5.40 | ppl   221.34\n",
      "| epoch   8 |   300/  696 batches | lr 30.00 | ms/batch 1414.91 | loss  5.36 | ppl   212.34\n",
      "| epoch   8 |   310/  696 batches | lr 30.00 | ms/batch 1430.48 | loss  5.30 | ppl   200.62\n",
      "| epoch   8 |   320/  696 batches | lr 30.00 | ms/batch 1407.50 | loss  5.36 | ppl   211.82\n",
      "| epoch   8 |   330/  696 batches | lr 30.00 | ms/batch 1436.19 | loss  5.34 | ppl   208.29\n",
      "| epoch   8 |   340/  696 batches | lr 30.00 | ms/batch 1428.44 | loss  5.31 | ppl   203.15\n",
      "| epoch   8 |   350/  696 batches | lr 30.00 | ms/batch 1442.98 | loss  5.33 | ppl   206.26\n",
      "| epoch   8 |   360/  696 batches | lr 30.00 | ms/batch 1499.32 | loss  5.28 | ppl   197.30\n",
      "| epoch   8 |   370/  696 batches | lr 30.00 | ms/batch 1506.58 | loss  5.32 | ppl   203.58\n",
      "| epoch   8 |   380/  696 batches | lr 30.00 | ms/batch 1470.45 | loss  5.29 | ppl   197.37\n",
      "| epoch   8 |   390/  696 batches | lr 30.00 | ms/batch 1413.22 | loss  5.29 | ppl   197.53\n",
      "| epoch   8 |   400/  696 batches | lr 30.00 | ms/batch 1439.86 | loss  5.28 | ppl   197.15\n",
      "| epoch   8 |   410/  696 batches | lr 30.00 | ms/batch 1450.70 | loss  5.36 | ppl   212.07\n",
      "| epoch   8 |   420/  696 batches | lr 30.00 | ms/batch 1611.14 | loss  5.29 | ppl   198.52\n",
      "| epoch   8 |   430/  696 batches | lr 30.00 | ms/batch 1450.61 | loss  5.30 | ppl   199.88\n",
      "| epoch   8 |   440/  696 batches | lr 30.00 | ms/batch 1612.97 | loss  5.26 | ppl   191.93\n",
      "| epoch   8 |   450/  696 batches | lr 30.00 | ms/batch 1584.94 | loss  5.32 | ppl   205.20\n",
      "| epoch   8 |   460/  696 batches | lr 30.00 | ms/batch 1691.99 | loss  5.29 | ppl   197.77\n",
      "| epoch   8 |   470/  696 batches | lr 30.00 | ms/batch 1595.32 | loss  5.30 | ppl   201.29\n",
      "| epoch   8 |   480/  696 batches | lr 30.00 | ms/batch 1470.12 | loss  5.28 | ppl   196.10\n",
      "| epoch   8 |   490/  696 batches | lr 30.00 | ms/batch 1439.77 | loss  5.27 | ppl   194.47\n",
      "| epoch   8 |   500/  696 batches | lr 30.00 | ms/batch 1415.15 | loss  5.26 | ppl   193.15\n",
      "| epoch   8 |   510/  696 batches | lr 30.00 | ms/batch 1477.72 | loss  5.23 | ppl   187.01\n",
      "| epoch   8 |   520/  696 batches | lr 30.00 | ms/batch 1442.13 | loss  5.24 | ppl   189.38\n",
      "| epoch   8 |   530/  696 batches | lr 30.00 | ms/batch 1417.58 | loss  5.21 | ppl   183.70\n",
      "| epoch   8 |   540/  696 batches | lr 30.00 | ms/batch 1413.25 | loss  5.24 | ppl   188.14\n",
      "| epoch   8 |   550/  696 batches | lr 30.00 | ms/batch 1521.21 | loss  5.26 | ppl   193.39\n",
      "| epoch   8 |   560/  696 batches | lr 30.00 | ms/batch 1555.83 | loss  5.29 | ppl   198.78\n",
      "| epoch   8 |   570/  696 batches | lr 30.00 | ms/batch 1577.11 | loss  5.26 | ppl   192.88\n",
      "| epoch   8 |   580/  696 batches | lr 30.00 | ms/batch 1741.80 | loss  5.26 | ppl   192.44\n",
      "| epoch   8 |   590/  696 batches | lr 30.00 | ms/batch 1549.48 | loss  5.27 | ppl   195.16\n",
      "| epoch   8 |   600/  696 batches | lr 30.00 | ms/batch 1579.17 | loss  5.25 | ppl   191.17\n",
      "| epoch   8 |   610/  696 batches | lr 30.00 | ms/batch 1565.90 | loss  5.18 | ppl   178.22\n",
      "| epoch   8 |   620/  696 batches | lr 30.00 | ms/batch 1477.74 | loss  5.27 | ppl   194.86\n",
      "| epoch   8 |   630/  696 batches | lr 30.00 | ms/batch 1578.32 | loss  5.28 | ppl   197.07\n",
      "| epoch   8 |   640/  696 batches | lr 30.00 | ms/batch 1594.58 | loss  5.25 | ppl   190.10\n",
      "| epoch   8 |   650/  696 batches | lr 30.00 | ms/batch 1539.36 | loss  5.25 | ppl   190.51\n",
      "| epoch   8 |   660/  696 batches | lr 30.00 | ms/batch 1600.34 | loss  5.28 | ppl   196.35\n",
      "| epoch   8 |   670/  696 batches | lr 30.00 | ms/batch 1708.87 | loss  5.28 | ppl   195.79\n",
      "| epoch   8 |   680/  696 batches | lr 30.00 | ms/batch 1564.19 | loss  5.25 | ppl   190.97\n",
      "| epoch   8 |   690/  696 batches | lr 30.00 | ms/batch 1778.50 | loss  5.23 | ppl   187.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 1102.61s | valid loss  5.18 | valid ppl   178.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    10/  696 batches | lr 30.00 | ms/batch 1563.46 | loss  5.87 | ppl   355.74\n",
      "| epoch   9 |    20/  696 batches | lr 30.00 | ms/batch 1429.73 | loss  5.31 | ppl   202.12\n",
      "| epoch   9 |    30/  696 batches | lr 30.00 | ms/batch 1436.38 | loss  5.29 | ppl   198.75\n",
      "| epoch   9 |    40/  696 batches | lr 30.00 | ms/batch 1414.43 | loss  5.33 | ppl   207.23\n",
      "| epoch   9 |    50/  696 batches | lr 30.00 | ms/batch 1430.76 | loss  5.27 | ppl   195.11\n",
      "| epoch   9 |    60/  696 batches | lr 30.00 | ms/batch 1452.06 | loss  5.27 | ppl   195.16\n",
      "| epoch   9 |    70/  696 batches | lr 30.00 | ms/batch 1415.00 | loss  5.26 | ppl   192.32\n",
      "| epoch   9 |    80/  696 batches | lr 30.00 | ms/batch 1450.42 | loss  5.33 | ppl   206.68\n",
      "| epoch   9 |    90/  696 batches | lr 30.00 | ms/batch 1414.58 | loss  5.30 | ppl   200.55\n",
      "| epoch   9 |   100/  696 batches | lr 30.00 | ms/batch 1422.85 | loss  5.34 | ppl   208.91\n",
      "| epoch   9 |   110/  696 batches | lr 30.00 | ms/batch 1450.93 | loss  5.31 | ppl   202.19\n",
      "| epoch   9 |   120/  696 batches | lr 30.00 | ms/batch 1519.94 | loss  5.29 | ppl   199.02\n",
      "| epoch   9 |   130/  696 batches | lr 30.00 | ms/batch 1431.87 | loss  5.25 | ppl   191.47\n",
      "| epoch   9 |   140/  696 batches | lr 30.00 | ms/batch 1748.62 | loss  5.29 | ppl   198.57\n",
      "| epoch   9 |   150/  696 batches | lr 30.00 | ms/batch 1654.59 | loss  5.25 | ppl   190.99\n",
      "| epoch   9 |   160/  696 batches | lr 30.00 | ms/batch 1551.37 | loss  5.24 | ppl   188.11\n",
      "| epoch   9 |   170/  696 batches | lr 30.00 | ms/batch 1441.14 | loss  5.22 | ppl   185.14\n",
      "| epoch   9 |   180/  696 batches | lr 30.00 | ms/batch 1443.31 | loss  5.28 | ppl   197.06\n",
      "| epoch   9 |   190/  696 batches | lr 30.00 | ms/batch 1629.46 | loss  5.31 | ppl   202.11\n",
      "| epoch   9 |   200/  696 batches | lr 30.00 | ms/batch 1573.40 | loss  5.32 | ppl   205.33\n",
      "| epoch   9 |   210/  696 batches | lr 30.00 | ms/batch 1426.12 | loss  5.30 | ppl   200.90\n",
      "| epoch   9 |   220/  696 batches | lr 30.00 | ms/batch 1423.25 | loss  5.26 | ppl   193.16\n",
      "| epoch   9 |   230/  696 batches | lr 30.00 | ms/batch 1621.87 | loss  5.33 | ppl   207.10\n",
      "| epoch   9 |   240/  696 batches | lr 30.00 | ms/batch 1456.69 | loss  5.29 | ppl   198.67\n",
      "| epoch   9 |   250/  696 batches | lr 30.00 | ms/batch 1540.75 | loss  5.28 | ppl   195.79\n",
      "| epoch   9 |   260/  696 batches | lr 30.00 | ms/batch 1518.37 | loss  5.24 | ppl   189.48\n",
      "| epoch   9 |   270/  696 batches | lr 30.00 | ms/batch 1453.76 | loss  5.31 | ppl   202.96\n",
      "| epoch   9 |   280/  696 batches | lr 30.00 | ms/batch 1411.91 | loss  5.26 | ppl   192.55\n",
      "| epoch   9 |   290/  696 batches | lr 30.00 | ms/batch 1439.91 | loss  5.33 | ppl   206.59\n",
      "| epoch   9 |   300/  696 batches | lr 30.00 | ms/batch 1575.07 | loss  5.31 | ppl   202.12\n",
      "| epoch   9 |   310/  696 batches | lr 30.00 | ms/batch 1612.85 | loss  5.26 | ppl   192.57\n",
      "| epoch   9 |   320/  696 batches | lr 30.00 | ms/batch 1712.97 | loss  5.31 | ppl   202.10\n",
      "| epoch   9 |   330/  696 batches | lr 30.00 | ms/batch 1492.30 | loss  5.29 | ppl   197.43\n",
      "| epoch   9 |   340/  696 batches | lr 30.00 | ms/batch 1622.51 | loss  5.26 | ppl   192.07\n",
      "| epoch   9 |   350/  696 batches | lr 30.00 | ms/batch 1798.34 | loss  5.26 | ppl   193.21\n",
      "| epoch   9 |   360/  696 batches | lr 30.00 | ms/batch 1860.53 | loss  5.23 | ppl   186.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |   370/  696 batches | lr 30.00 | ms/batch 1905.94 | loss  5.28 | ppl   195.75\n",
      "| epoch   9 |   380/  696 batches | lr 30.00 | ms/batch 1593.92 | loss  5.24 | ppl   189.61\n",
      "| epoch   9 |   390/  696 batches | lr 30.00 | ms/batch 1477.15 | loss  5.23 | ppl   186.80\n",
      "| epoch   9 |   400/  696 batches | lr 30.00 | ms/batch 1436.77 | loss  5.26 | ppl   191.86\n",
      "| epoch   9 |   410/  696 batches | lr 30.00 | ms/batch 1476.38 | loss  5.29 | ppl   197.78\n",
      "| epoch   9 |   420/  696 batches | lr 30.00 | ms/batch 1419.62 | loss  5.25 | ppl   189.67\n",
      "| epoch   9 |   430/  696 batches | lr 30.00 | ms/batch 1436.09 | loss  5.24 | ppl   187.90\n",
      "| epoch   9 |   440/  696 batches | lr 30.00 | ms/batch 1430.02 | loss  5.21 | ppl   183.23\n",
      "| epoch   9 |   450/  696 batches | lr 30.00 | ms/batch 1452.70 | loss  5.30 | ppl   200.44\n",
      "| epoch   9 |   460/  696 batches | lr 30.00 | ms/batch 1431.42 | loss  5.23 | ppl   186.86\n",
      "| epoch   9 |   470/  696 batches | lr 30.00 | ms/batch 1428.81 | loss  5.26 | ppl   193.11\n",
      "| epoch   9 |   480/  696 batches | lr 30.00 | ms/batch 1422.43 | loss  5.22 | ppl   184.85\n",
      "| epoch   9 |   490/  696 batches | lr 30.00 | ms/batch 1409.49 | loss  5.23 | ppl   186.23\n",
      "| epoch   9 |   500/  696 batches | lr 30.00 | ms/batch 1428.33 | loss  5.20 | ppl   181.39\n",
      "| epoch   9 |   510/  696 batches | lr 30.00 | ms/batch 1449.01 | loss  5.18 | ppl   177.69\n",
      "| epoch   9 |   520/  696 batches | lr 30.00 | ms/batch 1424.56 | loss  5.19 | ppl   179.81\n",
      "| epoch   9 |   530/  696 batches | lr 30.00 | ms/batch 1462.11 | loss  5.16 | ppl   173.89\n",
      "| epoch   9 |   540/  696 batches | lr 30.00 | ms/batch 1464.49 | loss  5.20 | ppl   180.43\n",
      "| epoch   9 |   550/  696 batches | lr 30.00 | ms/batch 1442.83 | loss  5.22 | ppl   184.48\n",
      "| epoch   9 |   560/  696 batches | lr 30.00 | ms/batch 1416.67 | loss  5.25 | ppl   189.77\n",
      "| epoch   9 |   570/  696 batches | lr 30.00 | ms/batch 1423.38 | loss  5.23 | ppl   186.20\n",
      "| epoch   9 |   580/  696 batches | lr 30.00 | ms/batch 1437.17 | loss  5.21 | ppl   183.73\n",
      "| epoch   9 |   590/  696 batches | lr 30.00 | ms/batch 1422.92 | loss  5.23 | ppl   187.65\n",
      "| epoch   9 |   600/  696 batches | lr 30.00 | ms/batch 1691.42 | loss  5.21 | ppl   182.26\n",
      "| epoch   9 |   610/  696 batches | lr 30.00 | ms/batch 1814.94 | loss  5.14 | ppl   171.51\n",
      "| epoch   9 |   620/  696 batches | lr 30.00 | ms/batch 1586.44 | loss  5.21 | ppl   183.38\n",
      "| epoch   9 |   630/  696 batches | lr 30.00 | ms/batch 1493.32 | loss  5.22 | ppl   185.48\n",
      "| epoch   9 |   640/  696 batches | lr 30.00 | ms/batch 1474.23 | loss  5.20 | ppl   181.09\n",
      "| epoch   9 |   650/  696 batches | lr 30.00 | ms/batch 1497.61 | loss  5.20 | ppl   181.45\n",
      "| epoch   9 |   660/  696 batches | lr 30.00 | ms/batch 1537.80 | loss  5.23 | ppl   186.54\n",
      "| epoch   9 |   670/  696 batches | lr 30.00 | ms/batch 1635.52 | loss  5.22 | ppl   184.18\n",
      "| epoch   9 |   680/  696 batches | lr 30.00 | ms/batch 1638.83 | loss  5.20 | ppl   181.86\n",
      "| epoch   9 |   690/  696 batches | lr 30.00 | ms/batch 1526.01 | loss  5.16 | ppl   174.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 1104.36s | valid loss  5.15 | valid ppl   171.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    10/  696 batches | lr 30.00 | ms/batch 1545.43 | loss  5.82 | ppl   337.50\n",
      "| epoch  10 |    20/  696 batches | lr 30.00 | ms/batch 1443.64 | loss  5.26 | ppl   192.65\n",
      "| epoch  10 |    30/  696 batches | lr 30.00 | ms/batch 1437.28 | loss  5.25 | ppl   191.18\n",
      "| epoch  10 |    40/  696 batches | lr 30.00 | ms/batch 1405.26 | loss  5.28 | ppl   195.41\n",
      "| epoch  10 |    50/  696 batches | lr 30.00 | ms/batch 1420.38 | loss  5.23 | ppl   186.05\n",
      "| epoch  10 |    60/  696 batches | lr 30.00 | ms/batch 1432.20 | loss  5.22 | ppl   185.83\n",
      "| epoch  10 |    70/  696 batches | lr 30.00 | ms/batch 1434.75 | loss  5.20 | ppl   181.84\n",
      "| epoch  10 |    80/  696 batches | lr 30.00 | ms/batch 1440.72 | loss  5.28 | ppl   196.49\n",
      "| epoch  10 |    90/  696 batches | lr 30.00 | ms/batch 1449.83 | loss  5.27 | ppl   194.91\n",
      "| epoch  10 |   100/  696 batches | lr 30.00 | ms/batch 1400.97 | loss  5.29 | ppl   197.62\n",
      "| epoch  10 |   110/  696 batches | lr 30.00 | ms/batch 1415.31 | loss  5.26 | ppl   192.91\n",
      "| epoch  10 |   120/  696 batches | lr 30.00 | ms/batch 1445.94 | loss  5.24 | ppl   188.89\n",
      "| epoch  10 |   130/  696 batches | lr 30.00 | ms/batch 1431.60 | loss  5.22 | ppl   185.57\n",
      "| epoch  10 |   140/  696 batches | lr 30.00 | ms/batch 1403.68 | loss  5.25 | ppl   189.79\n",
      "| epoch  10 |   150/  696 batches | lr 30.00 | ms/batch 1416.09 | loss  5.19 | ppl   179.52\n",
      "| epoch  10 |   160/  696 batches | lr 30.00 | ms/batch 1426.00 | loss  5.20 | ppl   181.38\n",
      "| epoch  10 |   170/  696 batches | lr 30.00 | ms/batch 1481.61 | loss  5.17 | ppl   176.33\n",
      "| epoch  10 |   180/  696 batches | lr 30.00 | ms/batch 1433.39 | loss  5.26 | ppl   191.91\n",
      "| epoch  10 |   190/  696 batches | lr 30.00 | ms/batch 1435.09 | loss  5.25 | ppl   190.55\n",
      "| epoch  10 |   200/  696 batches | lr 30.00 | ms/batch 1509.39 | loss  5.29 | ppl   197.84\n",
      "| epoch  10 |   210/  696 batches | lr 30.00 | ms/batch 1426.99 | loss  5.26 | ppl   192.34\n",
      "| epoch  10 |   220/  696 batches | lr 30.00 | ms/batch 1421.52 | loss  5.22 | ppl   185.85\n",
      "| epoch  10 |   230/  696 batches | lr 30.00 | ms/batch 1412.59 | loss  5.28 | ppl   197.05\n",
      "| epoch  10 |   240/  696 batches | lr 30.00 | ms/batch 1498.23 | loss  5.24 | ppl   189.21\n",
      "| epoch  10 |   250/  696 batches | lr 30.00 | ms/batch 1422.61 | loss  5.24 | ppl   187.81\n",
      "| epoch  10 |   260/  696 batches | lr 30.00 | ms/batch 1444.06 | loss  5.21 | ppl   182.30\n",
      "| epoch  10 |   270/  696 batches | lr 30.00 | ms/batch 1413.94 | loss  5.27 | ppl   193.76\n",
      "| epoch  10 |   280/  696 batches | lr 30.00 | ms/batch 1405.48 | loss  5.23 | ppl   187.49\n",
      "| epoch  10 |   290/  696 batches | lr 30.00 | ms/batch 1423.51 | loss  5.29 | ppl   198.51\n",
      "| epoch  10 |   300/  696 batches | lr 30.00 | ms/batch 1441.41 | loss  5.26 | ppl   193.08\n",
      "| epoch  10 |   310/  696 batches | lr 30.00 | ms/batch 1434.40 | loss  5.21 | ppl   182.29\n",
      "| epoch  10 |   320/  696 batches | lr 30.00 | ms/batch 1426.20 | loss  5.27 | ppl   194.91\n",
      "| epoch  10 |   330/  696 batches | lr 30.00 | ms/batch 1411.88 | loss  5.24 | ppl   189.06\n",
      "| epoch  10 |   340/  696 batches | lr 30.00 | ms/batch 1400.82 | loss  5.23 | ppl   187.40\n",
      "| epoch  10 |   350/  696 batches | lr 30.00 | ms/batch 1487.56 | loss  5.23 | ppl   186.74\n",
      "| epoch  10 |   360/  696 batches | lr 30.00 | ms/batch 1470.03 | loss  5.19 | ppl   178.99\n",
      "| epoch  10 |   370/  696 batches | lr 30.00 | ms/batch 1410.25 | loss  5.22 | ppl   184.92\n",
      "| epoch  10 |   380/  696 batches | lr 30.00 | ms/batch 1411.65 | loss  5.19 | ppl   180.12\n",
      "| epoch  10 |   390/  696 batches | lr 30.00 | ms/batch 1429.79 | loss  5.18 | ppl   177.56\n",
      "| epoch  10 |   400/  696 batches | lr 30.00 | ms/batch 1427.11 | loss  5.21 | ppl   182.26\n",
      "| epoch  10 |   410/  696 batches | lr 30.00 | ms/batch 1439.02 | loss  5.24 | ppl   188.13\n",
      "| epoch  10 |   420/  696 batches | lr 30.00 | ms/batch 1438.05 | loss  5.19 | ppl   178.90\n",
      "| epoch  10 |   430/  696 batches | lr 30.00 | ms/batch 1409.37 | loss  5.22 | ppl   184.28\n",
      "| epoch  10 |   440/  696 batches | lr 30.00 | ms/batch 1534.49 | loss  5.17 | ppl   175.30\n",
      "| epoch  10 |   450/  696 batches | lr 30.00 | ms/batch 1437.35 | loss  5.26 | ppl   192.43\n",
      "| epoch  10 |   460/  696 batches | lr 30.00 | ms/batch 1422.76 | loss  5.18 | ppl   177.85\n",
      "| epoch  10 |   470/  696 batches | lr 30.00 | ms/batch 1424.56 | loss  5.22 | ppl   184.93\n",
      "| epoch  10 |   480/  696 batches | lr 30.00 | ms/batch 1458.18 | loss  5.16 | ppl   175.03\n",
      "| epoch  10 |   490/  696 batches | lr 30.00 | ms/batch 1436.44 | loss  5.18 | ppl   178.31\n",
      "| epoch  10 |   500/  696 batches | lr 30.00 | ms/batch 1416.40 | loss  5.16 | ppl   174.26\n",
      "| epoch  10 |   510/  696 batches | lr 30.00 | ms/batch 1395.80 | loss  5.13 | ppl   168.37\n",
      "| epoch  10 |   520/  696 batches | lr 30.00 | ms/batch 1438.26 | loss  5.14 | ppl   170.60\n",
      "| epoch  10 |   530/  696 batches | lr 30.00 | ms/batch 1446.08 | loss  5.09 | ppl   161.97\n",
      "| epoch  10 |   540/  696 batches | lr 30.00 | ms/batch 1420.49 | loss  5.16 | ppl   174.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  10 |   550/  696 batches | lr 30.00 | ms/batch 1428.27 | loss  5.18 | ppl   177.19\n",
      "| epoch  10 |   560/  696 batches | lr 30.00 | ms/batch 1406.93 | loss  5.21 | ppl   182.51\n",
      "| epoch  10 |   570/  696 batches | lr 30.00 | ms/batch 1449.27 | loss  5.18 | ppl   178.32\n",
      "| epoch  10 |   580/  696 batches | lr 30.00 | ms/batch 1575.68 | loss  5.16 | ppl   173.67\n",
      "| epoch  10 |   590/  696 batches | lr 30.00 | ms/batch 1485.45 | loss  5.19 | ppl   179.19\n",
      "| epoch  10 |   600/  696 batches | lr 30.00 | ms/batch 1503.86 | loss  5.16 | ppl   173.94\n",
      "| epoch  10 |   610/  696 batches | lr 30.00 | ms/batch 1462.33 | loss  5.09 | ppl   162.76\n",
      "| epoch  10 |   620/  696 batches | lr 30.00 | ms/batch 1512.92 | loss  5.17 | ppl   175.91\n",
      "| epoch  10 |   630/  696 batches | lr 30.00 | ms/batch 1488.89 | loss  5.19 | ppl   180.03\n",
      "| epoch  10 |   640/  696 batches | lr 30.00 | ms/batch 1448.40 | loss  5.16 | ppl   174.23\n",
      "| epoch  10 |   650/  696 batches | lr 30.00 | ms/batch 1439.75 | loss  5.18 | ppl   177.30\n",
      "| epoch  10 |   660/  696 batches | lr 30.00 | ms/batch 1427.62 | loss  5.17 | ppl   176.44\n",
      "| epoch  10 |   670/  696 batches | lr 30.00 | ms/batch 1463.57 | loss  5.17 | ppl   176.07\n",
      "| epoch  10 |   680/  696 batches | lr 30.00 | ms/batch 1430.16 | loss  5.18 | ppl   176.84\n",
      "| epoch  10 |   690/  696 batches | lr 30.00 | ms/batch 1422.55 | loss  5.15 | ppl   172.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 1049.94s | valid loss  5.16 | valid ppl   174.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.08 | test ppl   160.74\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What do you think about\"\n",
    "\n",
    "# Transform new text into a tensor to give as input to the model\n",
    "words = text.split()\n",
    "ids = torch.LongTensor(len(words))\n",
    "for token, word in enumerate(words):\n",
    "    ids[token] = corpus.dictionary.word2idx[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4057],\n",
      "        [ 421],\n",
      "        [4481],\n",
      "        [6014],\n",
      "        [ 722]])\n"
     ]
    }
   ],
   "source": [
    "# We want the tensor to have the same dimensions as before : (sequence_len, batch_size)\n",
    "ids = ids.unsqueeze(-1)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: What do you think about was and 's have the have 's had to have you 's have of .\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of generated words, the temperature, the top-k words to sample from, and initiate the hidden state\n",
    "n_gen = 15\n",
    "temperature = 0.7\n",
    "k = 200\n",
    "hidden = model.init_hidden(ids.shape[1])\n",
    "\n",
    "for i in range(n_gen):\n",
    "    output, hidden = model(ids, hidden)\n",
    "    word_scores = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_scores[:k], 1)[0]\n",
    "    ids = torch.cat((ids, word_idx.unsqueeze(-1)), dim=0)\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "print('Generated text: ' + ' '.join([corpus.dictionary.idx2word[idx] for idx in ids]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
