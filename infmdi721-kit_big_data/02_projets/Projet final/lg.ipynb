{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b059a487",
   "metadata": {},
   "source": [
    "# Projet final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f940120",
   "metadata": {},
   "source": [
    "### Acquisition et chargement des données\n",
    "\n",
    "* Récupération des fichiers Excel avec les classements\n",
    "* Mise en place d'une copie locale des fichiers Excel afin de ne pas les recharger à chaque run.\n",
    "* Vers la fin de la course le format des fichiers Excel change avec les arrivées des voiliers : il est possible de s'arrêter juste avant.\n",
    "* Extraction des caractéristiques techniques de chacun des voiliers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e431acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des libs\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import datetime as dt\n",
    "import dateparser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# libs for xlsx \"xxid\" fix\n",
    "import tempfile\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "from fnmatch import fnmatch\n",
    "import re\n",
    "import glob\n",
    "#\n",
    "\n",
    "# variables utiliséees globalement\n",
    "URL_RESULTS=\"https://www.vendeeglobe.org/fr/classement\"\n",
    "FILE_RESULTS=\"classement.html\"\n",
    "URL_GLOSSAIRE=\"https://www.vendeeglobe.org/fr/glossaire\"\n",
    "FILE_GLOSSAIRE=\"glossaire.html\"\n",
    "EXCELS_DIR=\"results\"\n",
    "PICKLE_DF=\"pickle_classement.pkl\"\n",
    "PICKLE_DF_TECH=\"pickle_classement_et_tech.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69a1fb",
   "metadata": {},
   "source": [
    "\n",
    "* Récupération des fichiers Excel avec les classements\n",
    "* Mise en place d'une copie locale des fichiers Excel afin de ne pas les recharger à chaque run.\n",
    "* Chargement dans un dataframe (clean up et split des noms de colonne, préparation des data pour traitements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb7d7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_from_url(url):\n",
    "    \"\"\"\n",
    "    Retourne la soupe de l'url du fichier html passé en paramètre\n",
    "    \"\"\"\n",
    "    res = requests.get(url)\n",
    "    soup = bs(res.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_soup_from_file(file):\n",
    "    \"\"\"\n",
    "    Retourne la soupe du fichier html passé en paramètre\n",
    "    \"\"\"\n",
    "    soup = bs(file, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def parse_url_for_excels(url):\n",
    "    \"\"\"\n",
    "    Récupère la liste des fichiers excel et les télécharge dans le répertoire \"results/\" (EXCELS_DIR)\n",
    "    \"\"\"\n",
    "    print(\"Getting url of files to download...\")\n",
    "    soup = get_soup_from_url(url)\n",
    "    dates_list = []\n",
    "    for option in soup.find_all('option'):\n",
    "        if option['value'] != '':\n",
    "            dates_list.append(option['value'])\n",
    "    \n",
    "    # format de fichiers à récupérer \n",
    "    # https://www.vendeeglobe.org/download-race-data/vendeeglobe_20210305_080000.xlsx\n",
    "    print(\"Downloading xlsx files...\")\n",
    "    for date in dates_list:\n",
    "        xlsx_name = f'vendeeglobe_{date}.xlsx'\n",
    "        xlsx_file = requests.get(f\"https://www.vendeeglobe.org//download-race-data/{xlsx_name}\")\n",
    "        open(os.path.join(EXCELS_DIR, xlsx_name), 'wb').write(xlsx_file.content)\n",
    "\n",
    "def fix_xlsx_errors():\n",
    "    \"\"\"\n",
    "    Fix des fichiers xlsx, un header xxid dans un des fichiers du xlsx n'est pas reconnu par openpyxl\n",
    "    \"\"\"\n",
    "    print(\"Fixing xlsx files...\")\n",
    "    for file in [f for f in glob.glob(EXCELS_DIR + \"/*.xlsx\")]:\n",
    "        change_in_zip(file, name_filter='xl/styles.xml', # the problematic property is found in the style xml files\n",
    "                      change=lambda d: re.sub(b'xxid=\"\\d*\"', b\"\", d))\n",
    "        \n",
    "# fix of xlsx files\n",
    "def change_in_zip(file_name, name_filter, change):\n",
    "    \"\"\"\n",
    "    le fix appliqué à chaque fichier\n",
    "    \"\"\"\n",
    "    tempdir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        tempname = os.path.join(tempdir, 'new.zip')\n",
    "        with ZipFile(file_name, 'r') as r, ZipFile(tempname, 'w') as w:\n",
    "            for item in r.infolist():\n",
    "                data = r.read(item.filename)          \n",
    "                data = change(data)\n",
    "                w.writestr(item, data)\n",
    "        shutil.move(tempname, file_name)\n",
    "    finally:\n",
    "        shutil.rmtree(tempdir)\n",
    "\n",
    "\n",
    "def get_excel_files():\n",
    "    if not os.path.isdir(EXCELS_DIR):\n",
    "        os.mkdir('results')\n",
    "        print(\"Files being downloaded to \", EXCELS_DIR)\n",
    "        parse_url_for_excels(URL_RESULTS)\n",
    "        fix_xlsx_errors()\n",
    "    else:\n",
    "        print(f\"Les fichiers sont déjà dans le répertoire \\\"{EXCELS_DIR}/\\\" et déjà traités, aucun nouveau fichier téléchargé ni traité.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3a0e4",
   "metadata": {},
   "source": [
    "**L'appel à la commande *get_excel_files.xlsx* ne fait rien si le répertoire *results/* existe. Il est crée lors du 1er téléchargement** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190fc018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les fichiers sont déjà dans le répertoire \"results/\" et déjà traités, aucun nouveau fichier téléchargé ni traité.\n"
     ]
    }
   ],
   "source": [
    "get_excel_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813ebc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from pickle file pickle_classement.pkl... done\n",
      "\n",
      "(Note: if you need to reset pickle content remove file pickle_classement.pkl manually)\n",
      "CPU times: user 17.6 ms, sys: 4.11 ms, total: 21.7 ms\n",
      "Wall time: 20.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13703, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def create_dataframe_from_files(path, verbose=True):\n",
    "    \"\"\"\n",
    "    Sélection des fichiers et concatenate dans une dataframe\n",
    "    \n",
    "    Returns a dataframe\n",
    "    \"\"\"\n",
    "    if not verbose:\n",
    "        print(\"Quiet mode activated. Be patient...\", end='')\n",
    "    dfs=[]\n",
    "    for filename in [f for f in glob.glob(path + \"/*.xlsx\")]:\n",
    "        # on exclu les fichiers donnant des infos sur les concurrents arrivées (à partir du 27 janvier)\n",
    "        # on exclu le 1er fichier au départ qui est sans données\n",
    "        if filename >= path+'/vendeeglobe_20210127_170000.xlsx' or filename == path+\"/vendeeglobe_20201108_120200.xlsx\":\n",
    "            continue\n",
    "        if verbose:\n",
    "            print('Including file ', filename)\n",
    "\n",
    "        # read excel\n",
    "        x = pd.read_excel(filename, \n",
    "                          dtype=object,\n",
    "                          skiprows=[1, 2, 3], \n",
    "                          header=1, \n",
    "                          usecols=range(1,21), \n",
    "                          skipfooter=4)\n",
    "        # do not use col 0\n",
    "        # remove footer\n",
    "        # rename 1  Classement\n",
    "        # split 2 on \\n rename Pays /  Voile\n",
    "        # split 3 on \\n ren Skipper /  Bateau\n",
    "        # change names\n",
    "\n",
    "        # ajout colonne avec le timestamp du fichier d'où est extrait la data\n",
    "        x['Fichier de resultats'] = filename[-20:-5]\n",
    "\n",
    "        dfs.append(x)\n",
    "    \n",
    "    df = pd.concat(dfs , ignore_index=True)\n",
    "    #df.duplicated().value_counts()\n",
    "\n",
    "    if not verbose:\n",
    "        print(\"done\")\n",
    "    return df\n",
    "\n",
    "def clean_data(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Cleanup des noms de colonnes\n",
    "    \"\"\"\n",
    "    print('Cleaning dataframe... ', end='')\n",
    "\n",
    "    # cleanup sur nom dde cols, etc.\n",
    "    df.rename(inplace=True, columns={'Unnamed: 1': 'Classement', 'Unnamed: 2': 'Pays', 'Unnamed: 3': 'Skipper', \n",
    "                                    'Unnamed: 19': 'DTF', 'Unnamed: 20': 'DTL', \"Heure FR\\nHour FR\": \"Heure FR\"})\n",
    "    df[['Pays','Voile']] = df[\"Pays\"].str.extract(\"(.*)\\n(.*)\").astype(str)\n",
    "    df[['Skipper','Bateau']] = df[\"Skipper\"].str.extract(\"(.*)\\n(.*)\").astype(str)\n",
    "    df[['Heure FR']] = df[\"Heure FR\"].str.extract(\"(.*)\\n.*\").astype(str)\n",
    "    df.rename(inplace=True, columns={'Latitude\\nLatitude': 'Latitude', \n",
    "                    'Longitude\\nLongitude': 'Longitude', \n",
    "                    'Cap\\nHeading': 'Cap 30m', 'Vitesse\\nSpeed': 'Vitesse 30m', \n",
    "                    'VMG\\nVMG': 'VMG 30m', 'Distance\\nDistance': 'Distance 30m',\n",
    "                    'Cap\\nHeading.1': 'Cap dernier', 'Vitesse\\nSpeed.1': 'Vitesse dernier', \n",
    "                    'VMG\\nVMG.1': 'VMG dernier', 'Distance\\nDistance.1': 'Distance dernier',\n",
    "                    'Cap\\nHeading.2': 'Cap 24h', 'Vitesse\\nSpeed.2': 'Vitesse 24h', \n",
    "                    'VMG\\nVMG.2': 'VMG 24h', 'Distance\\nDistance.2': 'Distance 24h'})\n",
    "\n",
    "    for col in ['Vitesse 30m','VMG 30m', 'Vitesse dernier', 'VMG dernier', 'Vitesse 24h', 'VMG 24h']:\n",
    "        df[[col]] = df[col].str.extract(\"(.*) kts\").astype(float)\n",
    "    for col in ['Distance 30m', 'Distance dernier', 'Distance 24h', 'DTF', 'DTL']:\n",
    "        df[[col]] = df[col].str.extract(\"(.*) nm\").astype(float)\n",
    "    for col in ['Cap 30m', 'Cap dernier', 'Cap 24h']:\n",
    "        df[[col]] = df[col].str.extract(\"(\\d*)°\").astype(float)\n",
    "\n",
    "    # classement des abandons trasnformé en int\n",
    "    df[['Classement']] = df[['Classement']].applymap(lambda x: x.replace('RET', '-1')).astype(str)\n",
    "    df[['Classement']] = df[['Classement']].applymap(lambda x: x.replace('NL', '-2')).astype(str)\n",
    "    \n",
    "    \n",
    "    # suppression des lignes avec Nan\n",
    "    # ce sont les abandons\n",
    "    df = df.dropna()\n",
    "\n",
    "    print(\"done\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# backup dans un fichier pickle pour ne pas retraiter à chaque run\n",
    "if not os.path.isfile(PICKLE_DF):\n",
    "    print(f\"\\nLoading data from {EXCELS_DIR}/*.xlsx files\")\n",
    "    df = create_dataframe_from_files(EXCELS_DIR, verbose=False)\n",
    "    df = clean_data(df)\n",
    "    df.to_pickle(PICKLE_DF)\n",
    "    print(f\"{PICKLE_DF} saved\")\n",
    "else:\n",
    "    print(f\"\\nLoading data from pickle file {PICKLE_DF}...\", end='')\n",
    "    df = pd.read_pickle(PICKLE_DF)\n",
    "    print(' done')\n",
    "    print(f'\\n(Note: if you need to reset pickle content remove file {PICKLE_DF} manually)')\n",
    "    \n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4335844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classement                                     29\n",
       "Pays                                             \n",
       "Skipper                            Clément Giraud\n",
       "Heure FR                                 12:30 FR\n",
       "Latitude                               38°56.43'N\n",
       "Longitude                              19°07.68'W\n",
       "Cap 30m                                     228.0\n",
       "Vitesse 30m                                   4.2\n",
       "VMG 30m                                       3.3\n",
       "Distance 30m                                  2.1\n",
       "Cap dernier                                 177.0\n",
       "Vitesse dernier                               3.8\n",
       "VMG dernier                                   3.7\n",
       "Distance dernier                             11.4\n",
       "Cap 24h                                     241.0\n",
       "Vitesse 24h                                   8.2\n",
       "VMG 24h                                       5.7\n",
       "Distance 24h                                196.7\n",
       "DTF                                       23557.6\n",
       "DTL                                         370.4\n",
       "Fichier de resultats              20201113_110000\n",
       "Voile                                      FRA 83\n",
       "Bateau                  Compagnie du lit - Jiliti\n",
       "Name: 11578, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392482b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Extraction des caractéristiques techniques de chacun des voiliers.\n",
    "\n",
    "* Extraction des caractéristiques techniques de chacun des voiliers depuis la page glossaire \n",
    "* ajout des informations sur la présence de foils depuis la page classement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145f00e",
   "metadata": {},
   "source": [
    "Traitement manuel de la page web https://www.vendeeglobe.org/fr/classement pour récupérer l'info sur les foils et le classement final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa8b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infos_from_classement(df):\n",
    "    \"\"\"\n",
    "    Récupère les infos skipper, foil, etc.\n",
    "    \n",
    "    Retourne une liste de liste de [classement final, nom skipper, Oui/Non (foil)]\n",
    "    \"\"\"\n",
    "    print(\"Getting boats infos...\")\n",
    "    classement_html=\"\"\n",
    "    if not os.path.isfile(FILE_RESULTS):\n",
    "        print(\"Reading file from far away\")\n",
    "        req = requests.get(URL_RESULTS)\n",
    "        classement_html = req.content\n",
    "        open(os.path.join(FILE_RESULTS), 'wb').write(classement_html)\n",
    "    else:\n",
    "        print(\"Reading file locally\")\n",
    "        with open(FILE_RESULTS,'r') as file:\n",
    "            classement_html = file.read()\n",
    "\n",
    "    soup = get_soup_from_file(classement_html)\n",
    "    skippers_info=[]\n",
    "    for ranking_row in soup.find_all(\"tr\", {\"class\": \"ranking-row\"}):\n",
    "        cell_rank = ranking_row.find('td', attrs={'class': 'row-number'} ).text\n",
    "        cell_skipper = ranking_row.find('td', attrs={'class': 'row-skipper'} ).contents[2]\n",
    "        cell_skipper = re.search(r'\\n\\s+(\\w[\\s\\'\\w-]*)', cell_skipper).group(1).title()\n",
    "        cell_has_foil = ranking_row.find('td', attrs={'class': 'row-layout'} ).text\n",
    "        skippers_info.append([cell_rank, cell_skipper, cell_has_foil])\n",
    "    return skippers_info\n",
    "\n",
    "# Extraction des caractéristiques techniques de chacun des voiliers.\n",
    "def get_infos_from_glossaire():\n",
    "    \"\"\"\n",
    "    Récupère les infos skipper, foil, etc.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(FILE_GLOSSAIRE):\n",
    "        print(\"Reading file from far away...\" , end='')\n",
    "        req = requests.get(URL_GLOSSAIRE)\n",
    "        glossaire_html = req.content\n",
    "        open(os.path.join(FILE_GLOSSAIRE), 'wb').write(glossaire_html.content)\n",
    "    else:\n",
    "        print(\"Reading file locally... \" , end='')\n",
    "        with open(FILE_GLOSSAIRE,'r') as file:\n",
    "            glossaire_html = file.read()\n",
    "\n",
    "    soup = get_soup_from_file(glossaire_html)\n",
    "    \n",
    "    tech_info={}\n",
    "    \n",
    "    boats_popup_infos = soup.find_all('div', attrs={'class': 'boats-list__popup-infos'})\n",
    "    specs_list = soup.find_all('ul', attrs={'class': 'boats-list__popup-specs-list'})\n",
    "#     print(len(boats_popup_infos), len(specs_list))\n",
    "#     v = []\n",
    "    for i in range(len(boats_popup_infos)):\n",
    "        bateau = boats_popup_infos[i].h3.text\n",
    "        specs = specs_list[i]\n",
    "        voile = specs.find(string=re.compile(\"Numéro de voile : \"))\n",
    "        if voile == None:\n",
    "            voile = 0\n",
    "        else:\n",
    "            voile = re.match(\".*: ([\\w\\s]+)\", voile)[1]\n",
    "        anc_name = specs.find(string=re.compile(\"Anciens noms du bateau : \"))\n",
    "        if anc_name==None:\n",
    "            anc_name = bateau\n",
    "        else:    \n",
    "            anc_name = re.match(\".*: ([,\\w\\s]+)\", anc_name)[1]\n",
    "        Architecte = specs.find(string=re.compile(\"Architecte\")) # : Marc Lombard</li>\n",
    "        Architecte = re.match(\".*: ([\\s\\w]+)\", Architecte)[1]\n",
    "        Chantier= specs.find(string=re.compile(\"Chantier\")) #MAG France</li>\n",
    "        Chantier= re.match(\".*: ([\\s\\w]+)\", Chantier)[1]\n",
    "        lancement= specs.find(string=re.compile(\"Date de lancement\")) # : 01 Mars 1998</li>\n",
    "        lancement= re.match(\".*: ([\\s\\w]+)\", lancement)[1]\n",
    "        Longueur= specs.find(string=re.compile(\"Longueur\")) # : 18,28m</li>\n",
    "        Longueur= re.match(\".*: ([,.\\d]+)\", Longueur)[1]\n",
    "        Largeur = specs.find(string=re.compile(\"Largeur\")) # : 5,54m</li>\n",
    "        Largeur = re.match(\".*: ([,\\d]+)\", Largeur)[1]\n",
    "        Tirant = specs.find(string=re.compile(\"Tirant d'eau\")) # : 4,50m</li>\n",
    "        Tirant = re.match(\".*: ([,\\d]+)\", Tirant)[1]\n",
    "        poids = specs.find(string=re.compile(\"Déplacement\")) # : 9t</li>\n",
    "        poids = re.match(\".*: ([,\\dncNC]+)\\s?t?\", poids)[1]\n",
    "        if poids==\"nc\" or poids==\"NC\":\n",
    "            poids=\"0\"\n",
    "        derives = specs.find(string=re.compile(\"Nombre de dérives\")) # : 2</li>\n",
    "        derives = re.match(\".*: (.*)\", derives)[1]\n",
    "        mat = specs.find(string=re.compile(\"Hauteur mât\")) # : 29 m</li>\n",
    "        mat = re.match(\".*: ([,\\d]+)\", mat)[1]\n",
    "        quille = specs.find(string=re.compile(\"Voile quille\")) # : acier</li>\n",
    "        if quille == None:\n",
    "            quille = \"NC\"\n",
    "        else:\n",
    "            quille = re.match(\".*: ([\\s\\w]+)\", quille)[1] \n",
    "        Surface_pres = specs.find(string=re.compile(\"Surface de voiles au près\")) # : 260 m2</li>\n",
    "        Surface_pres = re.match(\".*: ([,\\d]+).*m[2²]\", Surface_pres)[1]\n",
    "        Surface_portant = specs.find(string=re.compile(\"Surface de voiles au portant\")) # : 580 m2</li>\n",
    "        Surface_portant = re.match(\".*: ([,\\d]+).*m[2²]\", Surface_portant)[1]\n",
    "\n",
    "        # manual cleanup des numéros de voiles qui ne matchent pas avec les fichiers classements\n",
    "        if bateau == 'LinkedOut':\n",
    "            voile = \"FRA 59\"\n",
    "        if voile == \"001\":\n",
    "            voile = \"FRA 01\"\n",
    "        if voile == \"4\":\n",
    "            voile = \"FRA 4\" \n",
    "        if voile == \"2\":\n",
    "            voile = \"FRA 02\"\n",
    "#         if voile == \"6\":\n",
    "#                 voile = \"FRA 6\"\n",
    "        if voile == \"08\":\n",
    "            voile = \"FRA 8\"\n",
    "        if voile == \"16\":\n",
    "            voile = \"MON 10\"\n",
    "        if voile == \"17\":\n",
    "            voile = \"FRA 17\"\n",
    "        if voile == \"18\":\n",
    "            voile = \"FRA 18\"\n",
    "        if voile == \"69\":\n",
    "            voile = \"FRA 69\"\n",
    "        if voile == \"SUI07\":\n",
    "            voile = \"SUI 7\"\n",
    "        if voile == \"GBR77\":\n",
    "            voile = \"GBR 777\"\n",
    "        if voile[3] != \" \":\n",
    "#             print(voile)\n",
    "            voile = voile[0:3]+\" \"+voile[3:]\n",
    "#             print(voile)\n",
    "        tech_info[voile] = {'Voile': voile, 'Nom bateau': bateau, 'Longeur': Longueur, \n",
    "                            'Largeur': Largeur, 'Tirant': Tirant, \n",
    "                           'Poids': poids, \"Dérives\": derives,\n",
    "                           'Hauteur mât': mat, \"Quille\": quille, \n",
    "                            \"Surface près\": Surface_pres,  \"Surface portant\": Surface_portant,\n",
    "                           \"Année lancement\": lancement[-4:], 'Ancien nom': anc_name}\n",
    "#         v.append(voile)\n",
    "    print(\"Done\")\n",
    "    return tech_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "284f72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting boats infos...\n",
      "Reading file locally\n",
      "{'Sam Davies', 'Arnaud Boissières', 'Alan  Roura'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "foils_etc = get_infos_from_classement(df)\n",
    "\n",
    "# manual fixes\n",
    "# skippers with names not matching\n",
    "# from web page 2 errors\n",
    "skips=set()\n",
    "for f in foils_etc:\n",
    "    skipper=f[1]\n",
    "    skips.add(skipper)\n",
    "all_skips = set(df['Skipper'])\n",
    "print(skips.difference(set(all_skips)))\n",
    "df[['Skipper']] = df[['Skipper']].applymap(lambda x: x.replace('Arnaud Boissieres', 'Arnaud Boissières')).astype(str)\n",
    "\n",
    "for f in foils_etc:\n",
    "    if f[1]==\"Sam Davies\":\n",
    "        skipper = \"Samantha Davies\"\n",
    "    elif f[1]==\"Alan  Roura\":\n",
    "        skipper=\"Alan Roura\"\n",
    "    else:\n",
    "        skipper=f[1]\n",
    "    foil=f[2]\n",
    "    \n",
    "    df.loc[df['Skipper']==skipper, 'Foil'] = foil\n",
    "    df.loc[df['Skipper']==skipper, 'Classement final'] = f[0]\n",
    "\n",
    "df[['Classement final']] = df[['Classement final']].applymap(lambda x: x.replace('ABD', '-1')).astype(str)\n",
    "df[['Classement final']] = df[['Classement final']].astype(int)\n",
    "df[['Classement']] = df[['Classement']].astype(int)\n",
    "\n",
    "\n",
    "# set(df['Skipper'])\n",
    "df[['Foil']] = df[['Foil']].applymap(lambda x: x.replace('Oui', '1')).astype(str)\n",
    "df[['Foil']] = df[['Foil']].applymap(lambda x: x.replace('Non', '0')).astype(str)\n",
    "df[['Foil']] = df[['Foil']].astype(int)\n",
    "df[['Foil']] = df[['Foil']].astype(bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fad97c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from pickle file pickle_classement_et_tech.pkl... done\n",
      "\n",
      "(Note: if you need to renew pickle content remove file pickle_classement_et_tech.pkl manually)\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "# MERGE DES CLASSEMENTS ET DONNEES TECHNIQUES\n",
    "\n",
    "\n",
    "# tech_info contains a dict of technical details\n",
    "# backup dans un fichier pickle pour ne pas retraiter à chaque run\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#brief-primer-on-merge-methods-relational-algebra\n",
    "\n",
    "def merge(df):\n",
    "    if not os.path.isfile(PICKLE_DF_TECH):  \n",
    "        print(f\"\\nLoading boats technical data\")\n",
    "        tech_info = get_infos_from_glossaire()\n",
    "\n",
    "        print(f\"\\nMerging df with technical data\")\n",
    "        df_tech_info = pd.DataFrame(tech_info).T\n",
    "        df_merge = pd.merge(df, df_tech_info, on=\"Voile\")\n",
    "\n",
    "        print(f\"Saving pickle file {PICKLE_DF_TECH}\")\n",
    "        df_merge.to_pickle(PICKLE_DF_TECH)\n",
    "    else:\n",
    "        print(f\"\\nLoading data from pickle file {PICKLE_DF_TECH}...\", end='')\n",
    "        df_merge = pd.read_pickle(PICKLE_DF_TECH)\n",
    "        print(' done')\n",
    "        print(f'\\n(Note: if you need to renew pickle content remove file {PICKLE_DF_TECH} manually)')\n",
    "    return df_merge\n",
    "\n",
    "df_merge = merge(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "995d4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleanup columns types, renaming and ordering\n",
    "\n",
    "df_merge['Tirant'] = df_merge[\"Tirant\"].str.replace(',','.').astype(float)\n",
    "df_merge['Longeur'] = df_merge[\"Longeur\"].str.replace(',','.').astype(float)\n",
    "df_merge['Largeur'] = df_merge[\"Largeur\"].str.replace(',','.').astype(float)\n",
    "df_merge['Poids'] = df_merge[\"Poids\"].str.replace(',','.').astype(float)\n",
    "df_merge[\"Hauteur mât\"] = df_merge[\"Hauteur mât\"].str.replace(',','.').astype(float)\n",
    "df_merge[\"Surface près\"] = df_merge[\"Surface près\"].str.replace(',','.').astype(float)\n",
    "df_merge[\"Surface portant\"] = df_merge[\"Surface portant\"].str.replace(',','.').astype(float)\n",
    "df_merge[\"Année lancement\"] = df_merge[\"Année lancement\"].astype(int)\n",
    "\n",
    "columns=['Classement', 'Classement final', 'Skipper', 'Bateau', 'Voile', 'Pays', 'Heure FR', 'Latitude', 'Longitude',\n",
    "       'Cap 30m', 'Vitesse 30m', 'VMG 30m', 'Distance 30m', 'Cap dernier',\n",
    "       'Vitesse dernier', 'VMG dernier', 'Distance dernier', 'Cap 24h',\n",
    "       'Vitesse 24h', 'VMG 24h', 'Distance 24h', 'DTF', 'DTL',\n",
    "       'Fichier de resultats', 'Foil', 'Dérives', 'Tirant', 'Longeur', 'Largeur', 'Poids', \n",
    "       'Hauteur mât', 'Quille', 'Surface près', 'Surface portant',\n",
    "       'Année lancement', 'Ancien nom']\n",
    "\n",
    "df_merge = df_merge[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9264b8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13703 entries, 0 to 13702\n",
      "Data columns (total 36 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Classement            13703 non-null  int64  \n",
      " 1   Classement final      13703 non-null  int64  \n",
      " 2   Skipper               13703 non-null  object \n",
      " 3   Bateau                13703 non-null  object \n",
      " 4   Voile                 13703 non-null  object \n",
      " 5   Pays                  13703 non-null  object \n",
      " 6   Heure FR              13703 non-null  object \n",
      " 7   Latitude              13703 non-null  object \n",
      " 8   Longitude             13703 non-null  object \n",
      " 9   Cap 30m               13703 non-null  float64\n",
      " 10  Vitesse 30m           13703 non-null  float64\n",
      " 11  VMG 30m               13703 non-null  float64\n",
      " 12  Distance 30m          13703 non-null  float64\n",
      " 13  Cap dernier           13703 non-null  float64\n",
      " 14  Vitesse dernier       13703 non-null  float64\n",
      " 15  VMG dernier           13703 non-null  float64\n",
      " 16  Distance dernier      13703 non-null  float64\n",
      " 17  Cap 24h               13703 non-null  float64\n",
      " 18  Vitesse 24h           13703 non-null  float64\n",
      " 19  VMG 24h               13703 non-null  float64\n",
      " 20  Distance 24h          13703 non-null  float64\n",
      " 21  DTF                   13703 non-null  float64\n",
      " 22  DTL                   13703 non-null  float64\n",
      " 23  Fichier de resultats  13703 non-null  object \n",
      " 24  Foil                  13703 non-null  bool   \n",
      " 25  Dérives               13703 non-null  object \n",
      " 26  Tirant                13703 non-null  float64\n",
      " 27  Longeur               13703 non-null  float64\n",
      " 28  Largeur               13703 non-null  float64\n",
      " 29  Poids                 13703 non-null  float64\n",
      " 30  Hauteur mât           13703 non-null  float64\n",
      " 31  Quille                13703 non-null  object \n",
      " 32  Surface près          13703 non-null  float64\n",
      " 33  Surface portant       13703 non-null  float64\n",
      " 34  Année lancement       13703 non-null  int64  \n",
      " 35  Ancien nom            13703 non-null  object \n",
      "dtypes: bool(1), float64(21), int64(3), object(11)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# check for Nan values\n",
    "# df.loc[df.isna().any(axis=1)]\n",
    "\n",
    "df_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "369b9411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classement                                  18\n",
       "Classement final                            11\n",
       "Skipper                           Armel Tripon\n",
       "Bateau                  L'Occitane en Provence\n",
       "Voile                                   FRA 02\n",
       "Pays                                        FR\n",
       "Heure FR                              15:00 FR\n",
       "Latitude                            46°18.29'N\n",
       "Longitude                           08°58.37'W\n",
       "Cap 30m                                  194.0\n",
       "Vitesse 30m                                9.8\n",
       "VMG 30m                                    9.7\n",
       "Distance 30m                               4.9\n",
       "Cap dernier                              195.0\n",
       "Vitesse dernier                           10.9\n",
       "VMG dernier                               10.9\n",
       "Distance dernier                          32.8\n",
       "Cap 24h                                  266.0\n",
       "Vitesse 24h                               12.4\n",
       "VMG 24h                                    7.1\n",
       "Distance 24h                             298.4\n",
       "DTF                                    24103.1\n",
       "DTL                                       32.6\n",
       "Fichier de resultats           20201109_140000\n",
       "Foil                                      True\n",
       "Dérives                                  foils\n",
       "Tirant                                     4.5\n",
       "Longeur                                  18.28\n",
       "Largeur                                    5.5\n",
       "Poids                                      7.8\n",
       "Hauteur mât                               28.0\n",
       "Quille                             acier forgé\n",
       "Surface près                             270.0\n",
       "Surface portant                          535.0\n",
       "Année lancement                           2020\n",
       "Ancien nom              L'OCCITANE EN PROVENCE\n",
       "Name: 10000, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.iloc[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d690cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### back to df \n",
    "\n",
    "df = df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733c90f",
   "metadata": {},
   "source": [
    "############################## AT WORK BELOW THIS LINE #####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d5a88",
   "metadata": {},
   "source": [
    "\n",
    "* Extraction des caractéristiques techniques de chacun des voiliers.\n",
    "* Rapprochement des données des voiliers avec celle des classements.\n",
    "* Corrélation et régression linéaire entre le classement (rang) et la vitesse utile (VMG) des voiliers.\n",
    "* Impact de la présence d'un foil sur le classement et la vitesse des voiliers.\n",
    "* Visualisation de la distance parcourue par voilier.\n",
    "* Cartes avec les routes d'un ou plusieurs voiliers.\n",
    "* Analyses de séries temporelles.\n",
    "* Application d'algorithmes statistiques ou de machine learning.\n",
    "* etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
